{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1) Install correct versions (restart may be prompted) ---\n!pip -q install \"torch>=2.4.0\" \"transformers>=4.51.3\" \"accelerate>=0.34.2\" sentencepiece\n\n# If the model is gated, login once (or set HF token env beforehand)\n# from huggingface_hub import login\n# login()  # paste your token when prompted\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T15:49:46.057251Z","iopub.execute_input":"2025-08-19T15:49:46.058199Z","iopub.status.idle":"2025-08-19T15:49:49.269655Z","shell.execute_reply.started":"2025-08-19T15:49:46.058165Z","shell.execute_reply":"2025-08-19T15:49:49.268627Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!pip -q install -U tqdm requests bitsandbytes sentencepiece einops safetensors openai huggingface_hub dropbox> /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T15:49:59.156002Z","iopub.execute_input":"2025-08-19T15:49:59.156287Z","iopub.status.idle":"2025-08-19T15:50:03.852512Z","shell.execute_reply.started":"2025-08-19T15:49:59.156261Z","shell.execute_reply":"2025-08-19T15:50:03.851499Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"DBX_APP_KEY\")\nsecret_value_1 = user_secrets.get_secret(\"DBX_APP_SECRET\")\nsecret_value_2 = user_secrets.get_secret(\"DBX_REFRESH_TOKEN\")\nsecret_value_3 = user_secrets.get_secret(\"GITHUB_PAT\")\nsecret_value_4 = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch, os, json, pathlib, gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T15:39:35.269858Z","iopub.execute_input":"2025-08-19T15:39:35.270257Z","iopub.status.idle":"2025-08-19T15:39:38.225832Z","shell.execute_reply.started":"2025-08-19T15:39:35.270220Z","shell.execute_reply":"2025-08-19T15:39:38.225273Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os, json, pathlib, posixpath, re, sys, time\nfrom typing import Optional, Iterable\nimport dropbox\nfrom dropbox.exceptions import AuthError, ApiError\nfrom google.colab import userdata\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T15:39:38.226778Z","iopub.execute_input":"2025-08-19T15:39:38.227132Z","iopub.status.idle":"2025-08-19T15:39:38.445499Z","shell.execute_reply.started":"2025-08-19T15:39:38.227103Z","shell.execute_reply":"2025-08-19T15:39:38.444985Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nDBX_APP_KEY = UserSecretsClient().get_secret(\"DBX_APP_KEY\")\nDBX_APP_SECRET = UserSecretsClient().get_secret(\"DBX_APP_SECRET\") \nDBX_REFRESH_TOKEN = UserSecretsClient().get_secret(\"DBX_REFRESH_TOKEN\")\nHF_TOKEN = UserSecretsClient().get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T15:39:38.446225Z","iopub.execute_input":"2025-08-19T15:39:38.446402Z","iopub.status.idle":"2025-08-19T15:39:39.452379Z","shell.execute_reply.started":"2025-08-19T15:39:38.446387Z","shell.execute_reply":"2025-08-19T15:39:39.451864Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import pathlib\n\nDBX_MODEL_DIR   = \"/p1-phi-3-mini-4k-instruct\"                # Dropbox folder path\nLOCAL_MODEL_DIR = \"/kaggle/working/phi-3-mini-4k-instruct\"    # Kaggle temp storage\n\n# Create the local directory if it doesn’t exist\npathlib.Path(LOCAL_MODEL_DIR).mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T15:18:49.788015Z","iopub.execute_input":"2025-08-19T15:18:49.788316Z","iopub.status.idle":"2025-08-19T15:18:49.792588Z","shell.execute_reply.started":"2025-08-19T15:18:49.788294Z","shell.execute_reply":"2025-08-19T15:18:49.791859Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token=HF_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T15:39:46.862997Z","iopub.execute_input":"2025-08-19T15:39:46.863286Z","iopub.status.idle":"2025-08-19T15:39:47.488538Z","shell.execute_reply.started":"2025-08-19T15:39:46.863264Z","shell.execute_reply":"2025-08-19T15:39:47.487961Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"google/gemma-3-1b-it\")\nmessages = [\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\npipe(messages)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T15:50:08.589388Z","iopub.execute_input":"2025-08-19T15:50:08.589688Z","iopub.status.idle":"2025-08-19T15:50:08.968213Z","shell.execute_reply.started":"2025-08-19T15:50:08.589662Z","shell.execute_reply":"2025-08-19T15:50:08.967270Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extra_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"open-llama\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"OpenLlama\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"openai-gpt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"OpenAI GPT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"opt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"OPT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'gemma3_text'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/4189770048.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"google/gemma-3-1b-it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m messages = [\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Who are you?\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentiment-analysis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0;31m# Question answering pipeline, specifying the checkpoint identifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m     >>> oracle = pipeline(\n\u001b[1;32m    807\u001b[0m     \u001b[0;34m...\u001b[0m     \u001b[0;34m\"question-answering\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"distilbert/distilbert-base-cased-distilled-squad\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"google-bert/bert-base-cased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 995\u001b[0;31m         \"\"\"\n\u001b[0m\u001b[1;32m    996\u001b[0m         \u001b[0mRegister\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mconfiguration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m         \"\"\"\n","\u001b[0;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `gemma3_text` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."],"ename":"ValueError","evalue":"The checkpoint you are trying to load has model type `gemma3_text` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")\nmessages = [\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\ninputs = tokenizer.apply_chat_template(\n\tmessages,\n\tadd_generation_prompt=True,\n\ttokenize=True,\n\treturn_dict=True,\n\treturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=40)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T15:20:29.204538Z","iopub.execute_input":"2025-08-19T15:20:29.205383Z","iopub.status.idle":"2025-08-19T15:21:20.235246Z","shell.execute_reply.started":"2025-08-19T15:20:29.205359Z","shell.execute_reply":"2025-08-19T15:21:20.234362Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a093664239ce4d3da3e3016edd361597"}},"metadata":{}},{"name":"stdout","text":"I am Phi, Microsoft's language model designed to assist with a wide range of queries and tasks. How can I help you today?<|end|>\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\nmodel_id = \"google/gemma-3-1b-it\"\n\n# 2) Load tokenizer & model\ntok = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,   # T4 supports bfloat16 in recent PyTorch\n    device_map=\"auto\"\n)\n\n# 3) Build a chat prompt via the model’s template\nmessages = [\n    {\"role\": \"user\", \"content\": \"Who are you?\"}\n]\nprompt = tok.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\n# 4) Generate (pipeline or raw generate both work)\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tok,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\n\nout = pipe(\n    prompt,\n    max_new_tokens=128,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    eos_token_id=tok.eos_token_id,\n    pad_token_id=tok.eos_token_id,\n)[0][\"generated_text\"]\n\nprint(out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T15:50:36.353936Z","iopub.execute_input":"2025-08-19T15:50:36.354441Z","iopub.status.idle":"2025-08-19T15:50:37.592685Z","shell.execute_reply.started":"2025-08-19T15:50:36.354421Z","shell.execute_reply":"2025-08-19T15:50:37.591713Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"489cad9ff024470c8bf19ca69f547e1d"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1602\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_liger_kernel_available\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/tokenization_gemma.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequires\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'requires' from 'transformers.utils.import_utils' (/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py)","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/4147046013.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 2) Load tokenizer & model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m model = AutoModelForCausalLM.from_pretrained(\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    863\u001b[0m     \u001b[0;31m# Save a pretrained tokenizer locally and you can reload its config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"google-bert/bert-base-cased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizer-test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mtokenizer_class_from_name\u001b[0;34m(class_name)\u001b[0m\n\u001b[1;32m    562\u001b[0m         (\n\u001b[1;32m    563\u001b[0m             \u001b[0;34m\"qwen3_moe\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             (\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0;34m\"Qwen2Tokenizer\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 \u001b[0;34m\"Qwen2TokenizerFast\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tokenizers_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_num2words_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1594\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_num2words_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"liger_kernel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0.3.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.gemma.tokenization_gemma because of the following error (look up to see its traceback):\ncannot import name 'requires' from 'transformers.utils.import_utils' (/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py)"],"ename":"RuntimeError","evalue":"Failed to import transformers.models.gemma.tokenization_gemma because of the following error (look up to see its traceback):\ncannot import name 'requires' from 'transformers.utils.import_utils' (/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py)","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T15:02:32.153470Z","iopub.execute_input":"2025-08-19T15:02:32.153790Z","iopub.status.idle":"2025-08-19T15:02:32.292277Z","shell.execute_reply.started":"2025-08-19T15:02:32.153759Z","shell.execute_reply":"2025-08-19T15:02:32.291025Z"}},"outputs":[{"name":"stdout","text":"gemma-3-1b-it  phi-3-mini-4k-instruct\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T15:03:48.739902Z","iopub.execute_input":"2025-08-19T15:03:48.740964Z","iopub.status.idle":"2025-08-19T15:03:48.874718Z","shell.execute_reply.started":"2025-08-19T15:03:48.740920Z","shell.execute_reply":"2025-08-19T15:03:48.873604Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"cd ..","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T15:03:47.566839Z","iopub.execute_input":"2025-08-19T15:03:47.567190Z","iopub.status.idle":"2025-08-19T15:03:47.574201Z","shell.execute_reply.started":"2025-08-19T15:03:47.567159Z","shell.execute_reply":"2025-08-19T15:03:47.573347Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# Pick one:\nmodel_id = \"microsoft/Phi-3-mini-4k-instruct\"\n# Alternatives:\n# model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n# model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n# model_id = \"google/gemma-2-2b-it\"\n# model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n# model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"  # borderline, keep short context\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\ntok = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\n# (Optional) small perf tweaks\ntorch.backends.cuda.matmul.allow_tf32 = True\n\n# Quick test\nprompt = \"You are a helpful assistant. Briefly explain what retrieval-augmented generation is.\"\ninputs = tok(prompt, return_tensors=\"pt\").to(model.device)\nout = model.generate(\n    **inputs,\n    max_new_tokens=200,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n)\nprint(tok.decode(out[0], skip_special_tokens=True))\n\n# See VRAM usage\nprint(\"Allocated (GB):\", round(torch.cuda.memory_allocated() / 1e9, 2))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}