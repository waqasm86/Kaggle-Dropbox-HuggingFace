{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install huggingface_hub > /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T07:29:35.174376Z","iopub.execute_input":"2025-08-21T07:29:35.174686Z","iopub.status.idle":"2025-08-21T07:29:38.189569Z","shell.execute_reply.started":"2025-08-21T07:29:35.174647Z","shell.execute_reply":"2025-08-21T07:29:38.188822Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!pip install openai > /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T07:29:38.191941Z","iopub.execute_input":"2025-08-21T07:29:38.192172Z","iopub.status.idle":"2025-08-21T07:29:41.524112Z","shell.execute_reply.started":"2025-08-21T07:29:38.192147Z","shell.execute_reply":"2025-08-21T07:29:41.523116Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from huggingface_hub import login, logout, whoami","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T07:29:41.525038Z","iopub.execute_input":"2025-08-21T07:29:41.525251Z","iopub.status.idle":"2025-08-21T07:29:41.529766Z","shell.execute_reply.started":"2025-08-21T07:29:41.525228Z","shell.execute_reply":"2025-08-21T07:29:41.529036Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nHF_TOKEN = UserSecretsClient().get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T07:29:41.530468Z","iopub.execute_input":"2025-08-21T07:29:41.530654Z","iopub.status.idle":"2025-08-21T07:29:45.507276Z","shell.execute_reply.started":"2025-08-21T07:29:41.530638Z","shell.execute_reply":"2025-08-21T07:29:45.506522Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"login(token=HF_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T07:29:45.508108Z","iopub.execute_input":"2025-08-21T07:29:45.508376Z","iopub.status.idle":"2025-08-21T07:29:45.554433Z","shell.execute_reply.started":"2025-08-21T07:29:45.508352Z","shell.execute_reply":"2025-08-21T07:29:45.553905Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# For GPU support\n!pip install -q -U transformers==4.45 accelerate bitsandbytes datasets einops > /dev/null\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T07:29:45.555002Z","iopub.execute_input":"2025-08-21T07:29:45.555183Z","iopub.status.idle":"2025-08-21T07:29:49.160864Z","shell.execute_reply.started":"2025-08-21T07:29:45.555169Z","shell.execute_reply":"2025-08-21T07:29:49.160071Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# Check if we have multiple GPUs\nprint(f\"Number of GPUs available: {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n\n# Configure 4-bit quantization to drastically reduce VRAM usage\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16 # Use FP16 for faster computation\n)\n\n# Load the model and let 'accelerate' automatically distribute it across available devices\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-3.1-8B-Instruct\",\n    quantization_config=quantization_config,\n    device_map=\"auto\",  # The key to multi-GPU and CPU offloading\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n# Add a padding token if it doesn't exist (for batch inference later)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Define the chat message\nmessages = [\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\n# Apply the chat template and prepare inputs\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_tensors=\"pt\", # Return PyTorch tensors\n)\n\n# Move inputs to the same device as the model's main layer\ninputs = inputs.to(model.device)\n\n# Generate a response\nwith torch.no_grad(): # Reduces memory usage during generation\n    outputs = model.generate(\n        input_ids=inputs,\n        max_new_tokens=256, # Increased for a more complete answer\n        do_sample=True, # Enable sampling for less repetitive answers\n        temperature=0.7, # Control randomness: lower is more deterministic\n        top_p=0.9, # Nucleus sampling: choose from top p probability mass\n        pad_token_id=tokenizer.eos_token_id # Set pad token for generation\n    )\n\n# Decode only the new tokens (skip the prompt)\nresponse = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\nprint(\"Model's response:\")\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:50:16.180704Z","iopub.execute_input":"2025-08-20T06:50:16.181494Z","iopub.status.idle":"2025-08-20T06:54:04.350322Z","shell.execute_reply.started":"2025-08-20T06:50:16.181463Z","shell.execute_reply":"2025-08-20T06:54:04.349532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###Approach 1###\n\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport gc\n\n# Check available devices\nprint(f\"Available GPUs: {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)} - Memory: {torch.cuda.get_device_properties(i).total_memory/1024**3:.1f}GB\")\n\n# Configuration\nmodel_name = \"defog/sqlcoder-7b-2\"\nmax_memory = {\n    0: \"12GB\",\n    1: \"12GB\",\n    \"cpu\": \"20GB\"\n}\n\n# Load tokenizer and set padding token\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token\n\n# Load model directly\nprint(\"Loading model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    max_memory=max_memory,\n    low_cpu_mem_usage=True\n)\n\nprint(f\"Model device map: {model.hf_device_map}\")\n\ndef generate_sql_query(question):\n    \"\"\"Generate SQL query using SQLCoder's preferred format\"\"\"\n    prompt = f\"\"\"### Task\nGenerate a SQL query to answer the following question:\n{question}\n\n### SQL\n\"\"\"\n    \n    # Tokenize input without padding\n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=512,\n        padding=False  # No padding needed for single sequences\n    )\n    \n    # Move inputs to the same device as the model\n    input_device = next(model.parameters()).device\n    inputs = {k: v.to(input_device) for k, v in inputs.items()}\n    \n    # Generate text\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            do_sample=True,\n            temperature=0.3,\n            pad_token_id=tokenizer.pad_token_id,  # Use the set pad token\n            repetition_penalty=1.1,\n            num_return_sequences=1\n        )\n    \n    # Extract only the generated SQL part (after the prompt)\n    generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n    return generated_text.strip()\n\n# Test queries\nqueries = [\n    \"Write a SQL query to find all customers who made purchases in the last 30 days.\",\n    \"Write a SQL query to find the top 5 products by sales volume.\",\n    \"Write a SQL query to calculate total revenue by month for the current year.\",\n    \"Write a SQL query to find customers who haven't made a purchase in the last 6 months.\"\n]\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"SQL QUERY GENERATION RESULTS\")\nprint(\"=\"*80)\n\nfor i, question in enumerate(queries, 1):\n    print(f\"\\n{i}. Question: {question}\")\n    try:\n        result = generate_sql_query(question)\n        print(f\"   Generated SQL:\\n   {result}\")\n    except Exception as e:\n        print(f\"   Error: {e}\")\n        import traceback\n        traceback.print_exc()\n\n# Memory usage info\nprint(f\"\\n\" + \"=\"*80)\nprint(\"MEMORY USAGE INFO\")\nprint(\"=\"*80)\nfor i in range(torch.cuda.device_count()):\n    allocated = torch.cuda.memory_allocated(i)/1024**3\n    cached = torch.cuda.memory_reserved(i)/1024**3\n    total = torch.cuda.get_device_properties(i).total_memory/1024**3\n    print(f\"GPU {i}: {allocated:.2f}GB allocated, {cached:.2f}GB cached / {total:.2f}GB total ({allocated/total*100:.1f}%)\")\n\n# Clean up\ndel model\ntorch.cuda.empty_cache()\ngc.collect()\n\nprint(f\"\\nAfter cleanup:\")\nfor i in range(torch.cuda.device_count()):\n    allocated = torch.cuda.memory_allocated(i)/1024**3\n    print(f\"GPU {i}: {allocated:.2f}GB allocated\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T05:55:28.290430Z","iopub.execute_input":"2025-08-21T05:55:28.291024Z","iopub.status.idle":"2025-08-21T05:55:51.079914Z","shell.execute_reply.started":"2025-08-21T05:55:28.291001Z","shell.execute_reply":"2025-08-21T05:55:51.079197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###Approach 1 Optimized###\n\n\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport gc\n\nclass SQLCoderGenerator:\n    def __init__(self, model_name=\"defog/sqlcoder-7b-2\"):\n        self.model_name = model_name\n        self.device_map = \"auto\"\n        self.max_memory = {0: \"12GB\", 1: \"12GB\", \"cpu\": \"20GB\"}\n        \n        # Load tokenizer and set padding token\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        # Load model\n        print(\"Loading SQLCoder model...\")\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            device_map=self.device_map,\n            torch_dtype=torch.float16,\n            max_memory=self.max_memory,\n            low_cpu_mem_usage=True\n        )\n        print(f\"Model loaded on devices: {self.model.hf_device_map}\")\n    \n    def generate_sql(self, question, max_new_tokens=150):\n        \"\"\"Generate SQL query for a given question\"\"\"\n        prompt = f\"\"\"### Task\nGenerate a SQL query to answer the following question:\n{question}\n\n### SQL\n\"\"\"\n        \n        # Tokenize input\n        inputs = self.tokenizer(\n            prompt,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=512,\n            padding=False\n        )\n        \n        # Move inputs to model device\n        input_device = next(self.model.parameters()).device\n        inputs = {k: v.to(input_device) for k, v in inputs.items()}\n        \n        # Generate SQL\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                do_sample=True,\n                temperature=0.3,\n                pad_token_id=self.tokenizer.pad_token_id,\n                repetition_penalty=1.1\n            )\n        \n        # Extract and clean the generated SQL\n        generated_text = self.tokenizer.decode(\n            outputs[0][inputs[\"input_ids\"].shape[1]:], \n            skip_special_tokens=True\n        )\n        return generated_text.strip()\n    \n    def cleanup(self):\n        \"\"\"Clean up memory\"\"\"\n        del self.model\n        torch.cuda.empty_cache()\n        gc.collect()\n\n# Usage example\nif __name__ == \"__main__\":\n    # Initialize generator\n    sql_generator = SQLCoderGenerator()\n    \n    # Test queries\n    test_queries = [\n        \"Write a SQL query to find all customers who made purchases in the last 30 days.\",\n        \"Write a SQL query to find the top 5 products by sales volume.\",\n        \"Write a SQL query to calculate total revenue by month for the current year.\",\n        \"Write a SQL query to find customers who haven't made a purchase in the last 6 months.\",\n        \"Write a SQL query to get the average order value by customer segment.\"\n    ]\n    \n    print(\"=\" * 80)\n    print(\"SQL CODER GENERATION RESULTS\")\n    print(\"=\" * 80)\n    \n    for i, question in enumerate(test_queries, 1):\n        print(f\"\\n{i}. {question}\")\n        sql_query = sql_generator.generate_sql(question)\n        print(f\"   \\033[92m{sql_query}\\033[0m\")  # Green color for SQL\n    \n    # Memory info\n    print(f\"\\n{'='*80}\")\n    print(\"MEMORY USAGE\")\n    print(f\"{'='*80}\")\n    for i in range(torch.cuda.device_count()):\n        allocated = torch.cuda.memory_allocated(i)/1024**3\n        total = torch.cuda.get_device_properties(i).total_memory/1024**3\n        print(f\"GPU {i}: {allocated:.2f}GB / {total:.2f}GB ({allocated/total*100:.1f}%)\")\n    \n    # Cleanup\n    sql_generator.cleanup()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T06:07:28.780516Z","iopub.execute_input":"2025-08-21T06:07:28.781240Z","iopub.status.idle":"2025-08-21T06:07:54.881157Z","shell.execute_reply.started":"2025-08-21T06:07:28.781214Z","shell.execute_reply":"2025-08-21T06:07:54.880610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport re\n\nclass SQLCoderGenerator:\n    def __init__(self, model_name=\"defog/sqlcoder-7b-2\"):\n        self.model_name = model_name\n        \n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n        print(\"Loading SQLCoder model...\")\n        # Load model with device map for multiple GPUs\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            device_map=\"auto\",\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True\n        )\n        self.model.eval()\n        print(f\"Model loaded on device: {self.model.device}\")\n\n    def create_prompt(self, question, schema_context):\n        prompt_template = f\"\"\"\n### Instructions:\nYour task is to write a SQL query to answer the following question.\nYour response must be ONLY the valid SQL query itself, without any explanation or text before or after.\n\n### Database Schema:\nThe query will run on a database with the following tables and columns:\n{schema_context}\n\n### Question:\n{question}\n\n### Answer:\n\"\"\"\n        return prompt_template.strip()\n\n    def generate_sql(self, question, schema_context, max_new_tokens=200):\n        prompt = self.create_prompt(question, schema_context)\n        \n        inputs = self.tokenizer(\n            prompt,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=2048,\n            padding=True\n        )\n        \n        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                do_sample=False,\n                temperature=0.3,\n                pad_token_id=self.tokenizer.pad_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n                repetition_penalty=1.1\n            )\n        \n        generated_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n        generated_sql = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n        \n        # Clean up the output\n        generated_sql = generated_sql.split(';')[0] + ';'\n        generated_sql = generated_sql.strip()\n        \n        return generated_sql\n\n# Usage remains the same\nsql_generator = SQLCoderGenerator()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T07:32:56.242627Z","iopub.execute_input":"2025-08-21T07:32:56.243477Z","iopub.status.idle":"2025-08-21T07:34:28.345090Z","shell.execute_reply.started":"2025-08-21T07:32:56.243436Z","shell.execute_reply":"2025-08-21T07:34:28.344516Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98d3df9f82d64acaa24ef49434765ea9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e58171fb021495e83e21672f3b8cede"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c34a7d3b35f641d38b579f73012dee78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/515 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0ddd9e4d219462ba35d719c87cda87e"}},"metadata":{}},{"name":"stdout","text":"Loading SQLCoder model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/691 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31748d96aece4452a3b95a3c85e620c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76358bcca24b4fd8967ed12bbd50e3b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58d3d79fea8a4fe7b034a87456cc4bcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a2152566c79454ab3506726c3c527b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"823df7fb58944457bfc1d0ab35610a9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de3fea78c8074f2a894e44e875f52db8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ff970df051e4b96909a6bb6b4a331a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d44753c7f1f547fc87f380eafb9abdbe"}},"metadata":{}},{"name":"stdout","text":"Model loaded on device: cuda:0\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport re\n\nclass SQLCoderGenerator:\n    def __init__(self, model_name=\"defog/sqlcoder-7b-2\"):\n        self.model_name = model_name\n        \n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        # Configure quantization\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.bfloat16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\"\n        )\n            \n        print(\"Loading SQLCoder model...\")\n        # Load model with quantization\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            device_map=\"auto\",\n            quantization_config=quantization_config,\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True\n        )\n        self.model.eval()\n        print(f\"Model loaded on device: {self.model.device}\")\n\n    def create_prompt(self, question, schema_context):\n        prompt_template = f\"\"\"\n### Instructions:\nYour task is to write a SQL query to answer the following question.\nYour response must be ONLY the valid SQL query itself, without any explanation or text before or after.\n\n### Database Schema:\nThe query will run on a database with the following tables and columns:\n{schema_context}\n\n### Question:\n{question}\n\n### Answer:\n\"\"\"\n        return prompt_template.strip()\n\n    def generate_sql(self, question, schema_context, max_new_tokens=200):\n        prompt = self.create_prompt(question, schema_context)\n        \n        inputs = self.tokenizer(\n            prompt,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=2048,\n            padding=True\n        )\n        \n        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                do_sample=False,\n                temperature=0.3,\n                pad_token_id=self.tokenizer.pad_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n                repetition_penalty=1.1\n            )\n        \n        generated_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n        generated_sql = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n        \n        # Clean up the output\n        generated_sql = generated_sql.split(';')[0] + ';'\n        generated_sql = generated_sql.strip()\n        \n        return generated_sql\n\n# Usage\nsql_generator = SQLCoderGenerator()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T07:37:41.913573Z","iopub.execute_input":"2025-08-21T07:37:41.914139Z","iopub.status.idle":"2025-08-21T07:38:06.031506Z","shell.execute_reply.started":"2025-08-21T07:37:41.914115Z","shell.execute_reply":"2025-08-21T07:38:06.030800Z"}},"outputs":[{"name":"stdout","text":"Loading SQLCoder model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34538054222b4ffb8f213be519a9d553"}},"metadata":{}},{"name":"stdout","text":"Model loaded on device: cuda:0\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Example schema context and question\nschema_context = \"\"\"\nCREATE TABLE users (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    email VARCHAR(150),\n    created_at TIMESTAMP\n);\n\nCREATE TABLE orders (\n    id INT PRIMARY KEY,\n    user_id INT,\n    amount DECIMAL(10,2),\n    order_date DATE,\n    FOREIGN KEY (user_id) REFERENCES users(id)\n);\n\"\"\"\n\nquestion = \"Find all users who have placed more than 5 orders\"\n\n# Generate SQL\ntry:\n    sql_query = sql_generator.generate_sql(question, schema_context)\n    print(\"Generated SQL:\")\n    print(sql_query)\nexcept Exception as e:\n    print(f\"Error generating SQL: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T07:43:47.326485Z","iopub.execute_input":"2025-08-21T07:43:47.327214Z","iopub.status.idle":"2025-08-21T07:44:07.373248Z","shell.execute_reply.started":"2025-08-21T07:43:47.327190Z","shell.execute_reply":"2025-08-21T07:44:07.372483Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n2025-08-21 07:43:51.243113: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755762231.456002      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755762231.515701      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Generated SQL:\nSELECT u.name FROM users u JOIN orders o ON u.id = o.user_id GROUP BY u.name HAVING COUNT(o.id) > 5;\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def generate_sql(self, question, schema_context, max_new_tokens=200):\n    prompt = self.create_prompt(question, schema_context)\n    \n    inputs = self.tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=2048,\n        padding=True\n    )\n    \n    inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = self.model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=False,\n            temperature=0.3,\n            pad_token_id=self.tokenizer.pad_token_id,\n            eos_token_id=self.tokenizer.eos_token_id,\n            repetition_penalty=1.1\n        )\n    \n    generated_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n    generated_sql = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n    \n    # Enhanced cleaning\n    generated_sql = generated_sql.strip()\n    \n    # Remove any markdown code blocks if present\n    generated_sql = re.sub(r'```sql\\s*', '', generated_sql)\n    generated_sql = re.sub(r'```\\s*', '', generated_sql)\n    \n    # Ensure it ends with semicolon\n    if not generated_sql.endswith(';'):\n        generated_sql += ';'\n    \n    # Extract only the first SQL statement\n    generated_sql = generated_sql.split(';')[0] + ';'\n    \n    return generated_sql","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T07:44:38.509236Z","iopub.execute_input":"2025-08-21T07:44:38.510331Z","iopub.status.idle":"2025-08-21T07:44:38.516429Z","shell.execute_reply.started":"2025-08-21T07:44:38.510281Z","shell.execute_reply":"2025-08-21T07:44:38.515775Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def generate_multiple_queries(self, questions_schemas_list):\n    \"\"\"Generate SQL for multiple question-schema pairs\"\"\"\n    results = []\n    for question, schema_context in questions_schemas_list:\n        try:\n            sql = self.generate_sql(question, schema_context)\n            results.append({\n                'question': question,\n                'schema': schema_context,\n                'generated_sql': sql,\n                'status': 'success'\n            })\n        except Exception as e:\n            results.append({\n                'question': question,\n                'schema': schema_context,\n                'generated_sql': None,\n                'status': f'error: {str(e)}'\n            })\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T07:44:48.748254Z","iopub.execute_input":"2025-08-21T07:44:48.749044Z","iopub.status.idle":"2025-08-21T07:44:48.753813Z","shell.execute_reply.started":"2025-08-21T07:44:48.749018Z","shell.execute_reply":"2025-08-21T07:44:48.752999Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def validate_sql(self, sql_query):\n    \"\"\"Basic SQL validation (can be extended)\"\"\"\n    if not sql_query:\n        return False, \"Empty query\"\n    \n    # Check if it looks like SQL\n    sql_keywords = ['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'FROM', 'WHERE', 'JOIN']\n    if not any(keyword in sql_query.upper() for keyword in sql_keywords):\n        return False, \"Doesn't appear to be SQL\"\n    \n    return True, \"Valid SQL\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T07:44:55.891571Z","iopub.execute_input":"2025-08-21T07:44:55.892239Z","iopub.status.idle":"2025-08-21T07:44:55.896405Z","shell.execute_reply.started":"2025-08-21T07:44:55.892217Z","shell.execute_reply":"2025-08-21T07:44:55.895589Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport re\nimport sqlite3\nimport pandas as pd\n\nclass SQLCoderGenerator:\n    def __init__(self, model_name=\"defog/sqlcoder-7b-2\"):\n        self.model_name = model_name\n        \n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        # Configure quantization\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.bfloat16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\"\n        )\n            \n        print(\"Loading SQLCoder model...\")\n        # Load model with quantization\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            device_map=\"auto\",\n            quantization_config=quantization_config,\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True\n        )\n        self.model.eval()\n        print(f\"Model loaded on device: {self.model.device}\")\n\n    def create_prompt(self, question, schema_context):\n        prompt_template = f\"\"\"\n### Instructions:\nYour task is to write a SQL query to answer the following question.\nYour response must be ONLY the valid SQL query itself, without any explanation or text before or after.\n\n### Database Schema:\nThe query will run on a database with the following tables and columns:\n{schema_context}\n\n### Question:\n{question}\n\n### Answer:\n\"\"\"\n        return prompt_template.strip()\n\n    def generate_sql(self, question, schema_context, max_new_tokens=200):\n        prompt = self.create_prompt(question, schema_context)\n        \n        inputs = self.tokenizer(\n            prompt,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=2048,\n            padding=True\n        )\n        \n        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                do_sample=False,\n                temperature=0.3,\n                pad_token_id=self.tokenizer.pad_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n                repetition_penalty=1.1\n            )\n        \n        generated_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n        generated_sql = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n        \n        # Enhanced cleaning\n        generated_sql = generated_sql.strip()\n        \n        # Remove any markdown code blocks if present\n        generated_sql = re.sub(r'```sql\\s*', '', generated_sql)\n        generated_sql = re.sub(r'```\\s*', '', generated_sql)\n        \n        # Ensure it ends with semicolon\n        if not generated_sql.endswith(';'):\n            generated_sql += ';'\n        \n        # Extract only the first SQL statement\n        generated_sql = generated_sql.split(';')[0] + ';'\n        \n        return generated_sql\n\n    def validate_sql(self, sql_query):\n        \"\"\"Basic SQL validation (can be extended)\"\"\"\n        if not sql_query or sql_query == ';':\n            return False, \"Empty query\"\n        \n        # Check if it looks like SQL\n        sql_keywords = ['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'FROM', 'WHERE', 'JOIN']\n        if not any(keyword in sql_query.upper() for keyword in sql_keywords):\n            return False, \"Doesn't appear to be SQL\"\n        \n        return True, \"Valid SQL\"\n\n    def test_sql_execution(self, sql_query, db_path=\":memory:\"):\n        \"\"\"Test if SQL can be executed (for simple cases)\"\"\"\n        try:\n            conn = sqlite3.connect(db_path)\n            # This is just for syntax checking - actual execution would need real data\n            pd.read_sql_query(\"EXPLAIN QUERY PLAN \" + sql_query, conn)\n            conn.close()\n            return True, \"SQL syntax appears valid\"\n        except Exception as e:\n            return False, f\"SQL execution error: {str(e)}\"\n\n# Initialize the generator\nsql_generator = SQLCoderGenerator()\n\n# Test cases\ntest_cases = [\n    {\n        \"schema\": \"\"\"\n        CREATE TABLE employees (\n            id INT PRIMARY KEY,\n            name VARCHAR(100),\n            department VARCHAR(50),\n            salary DECIMAL(10,2)\n        );\n        \"\"\",\n        \"question\": \"Find the average salary by department\"\n    },\n    {\n        \"schema\": \"\"\"\n        CREATE TABLE products (\n            id INT PRIMARY KEY,\n            name VARCHAR(200),\n            price DECIMAL(10,2),\n            category VARCHAR(50)\n        );\n        CREATE TABLE sales (\n            id INT PRIMARY KEY,\n            product_id INT,\n            quantity INT,\n            sale_date DATE,\n            FOREIGN KEY (product_id) REFERENCES products(id)\n        );\n        \"\"\",\n        \"question\": \"Find total sales amount for each product category in the last month\"\n    }\n]\n\n# Test the generator\nfor i, test_case in enumerate(test_cases):\n    print(f\"\\nTest Case {i+1}:\")\n    print(f\"Question: {test_case['question']}\")\n    \n    sql = sql_generator.generate_sql(test_case['question'], test_case['schema'])\n    print(f\"Generated SQL: {sql}\")\n    \n    is_valid, message = sql_generator.validate_sql(sql)\n    print(f\"Validation: {message}\")\n    \n    # Optional: Test SQL execution syntax\n    if is_valid and sql != ';':\n        exec_valid, exec_message = sql_generator.test_sql_execution(sql)\n        print(f\"Execution test: {exec_message}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T07:48:28.063416Z","iopub.execute_input":"2025-08-21T07:48:28.064086Z","iopub.status.idle":"2025-08-21T07:48:51.042909Z","shell.execute_reply.started":"2025-08-21T07:48:28.064060Z","shell.execute_reply":"2025-08-21T07:48:51.041906Z"}},"outputs":[{"name":"stdout","text":"Loading SQLCoder model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19aa3140b4c14dfabf8167e743e1a3f0"}},"metadata":{}},{"name":"stdout","text":"Model loaded on device: cuda:0\n\nTest Case 1:\nQuestion: Find the average salary by department\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Generated SQL: ;\nValidation: Empty query\n\nTest Case 2:\nQuestion: Find total sales amount for each product category in the last month\nGenerated SQL: ;\nValidation: Empty query\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}