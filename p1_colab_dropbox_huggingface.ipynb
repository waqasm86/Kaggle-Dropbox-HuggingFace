{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMc9Gp2fozFg0JykL92ShoR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fbb0c2edbd284a7ca3eb17bb7e7174d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c996db176f646cf94451041d732efee",
              "IPY_MODEL_6a7ee0ac8b314bcb83bce2c29f1e0c7e",
              "IPY_MODEL_d91027a9e7404f9d8a14363e01228c0f"
            ],
            "layout": "IPY_MODEL_de0d8c82a6914fc89a786ca725ad5790"
          }
        },
        "2c996db176f646cf94451041d732efee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_497af52380c4490d999e34c898bb890d",
            "placeholder": "​",
            "style": "IPY_MODEL_311e5183e773481f87aada22e6eac5f4",
            "value": "Loading checkpoint shards:  33%"
          }
        },
        "6a7ee0ac8b314bcb83bce2c29f1e0c7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b68698c2399a45c681157aa8e6e57263",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b622b7089c04de38f3655c5a65e39de",
            "value": 1
          }
        },
        "d91027a9e7404f9d8a14363e01228c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80af1edcde444a3f9a0dc6e78a1cc869",
            "placeholder": "​",
            "style": "IPY_MODEL_b136d62ef42640ae8f4f4864a33da513",
            "value": " 1/3 [00:12&lt;00:10,  5.40s/it]"
          }
        },
        "de0d8c82a6914fc89a786ca725ad5790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "497af52380c4490d999e34c898bb890d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "311e5183e773481f87aada22e6eac5f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b68698c2399a45c681157aa8e6e57263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b622b7089c04de38f3655c5a65e39de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "80af1edcde444a3f9a0dc6e78a1cc869": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b136d62ef42640ae8f4f4864a33da513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6639bab5ee914950b5e7fe6f9b9077af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f866aea43c444ba92c28dacf9742808",
              "IPY_MODEL_624c3c36f87547d1b60a93cd22a5561d",
              "IPY_MODEL_2e2e91fea0bd4d42954cea897c4f8588"
            ],
            "layout": "IPY_MODEL_e54cb87e450542f380bac337434609dd"
          }
        },
        "9f866aea43c444ba92c28dacf9742808": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee9342dad9d0475eb79472d18e4f7d21",
            "placeholder": "​",
            "style": "IPY_MODEL_1645fc514a734683bea4ec848b02361f",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "624c3c36f87547d1b60a93cd22a5561d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a94c12e0f824e28b6bc1301600774c4",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_921fad38a0ce4984afc2c46e5e7b888d",
            "value": 0
          }
        },
        "2e2e91fea0bd4d42954cea897c4f8588": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d82703da76b54a409745cbfe3cfcc1fb",
            "placeholder": "​",
            "style": "IPY_MODEL_064c0ce1e2de4bdda9aae7e3980263d9",
            "value": " 0/3 [00:00&lt;?, ?it/s]"
          }
        },
        "e54cb87e450542f380bac337434609dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee9342dad9d0475eb79472d18e4f7d21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1645fc514a734683bea4ec848b02361f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a94c12e0f824e28b6bc1301600774c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "921fad38a0ce4984afc2c46e5e7b888d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d82703da76b54a409745cbfe3cfcc1fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "064c0ce1e2de4bdda9aae7e3980263d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "558ecd2df3834effa3a0cd260048f5b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3a077c7486d4a5796301e7cd32a2f96",
              "IPY_MODEL_74ed77885b1444f1bbe230f9847c08a5",
              "IPY_MODEL_0f145c96fa314fdcadd73f0ab32ad02d"
            ],
            "layout": "IPY_MODEL_654e655bcff84fd18e819ee14fc3ff15"
          }
        },
        "b3a077c7486d4a5796301e7cd32a2f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee924c9a28c84d9a8cfe18bc27fee58e",
            "placeholder": "​",
            "style": "IPY_MODEL_0b1ff72178454a31b860ae6f1c67fb0e",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "74ed77885b1444f1bbe230f9847c08a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ea536aac2d34919b4b1533239b6d50b",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d00ca1e85fa477a9b68bdf9192c8710",
            "value": 0
          }
        },
        "0f145c96fa314fdcadd73f0ab32ad02d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c28d51be305d44eb96f8bd36bfa1b74b",
            "placeholder": "​",
            "style": "IPY_MODEL_063d421f811146d2b7d0abc4a2b1ee54",
            "value": " 0/3 [00:00&lt;?, ?it/s]"
          }
        },
        "654e655bcff84fd18e819ee14fc3ff15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee924c9a28c84d9a8cfe18bc27fee58e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b1ff72178454a31b860ae6f1c67fb0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ea536aac2d34919b4b1533239b6d50b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d00ca1e85fa477a9b68bdf9192c8710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c28d51be305d44eb96f8bd36bfa1b74b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "063d421f811146d2b7d0abc4a2b1ee54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae4abb107b93494b9cfb7b88c516bdc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43548a760dc940c3ae2d3845950f2640",
              "IPY_MODEL_733a600797be4934a2d4c69325800825",
              "IPY_MODEL_42fae67ae3764d039026592d0abc0941"
            ],
            "layout": "IPY_MODEL_4406c5e163fa448e9d386eae06edd792"
          }
        },
        "43548a760dc940c3ae2d3845950f2640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_652bff37386b4e12a53e030f0ee212bd",
            "placeholder": "​",
            "style": "IPY_MODEL_8ccd428d75f7483d8d40e867b31c4740",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "733a600797be4934a2d4c69325800825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_374d4e46382b4dc2bc0d7391a997e40f",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e060616c09d4f59a560397c1dccd527",
            "value": 0
          }
        },
        "42fae67ae3764d039026592d0abc0941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3938a447918d446392f3118e21c55745",
            "placeholder": "​",
            "style": "IPY_MODEL_9e6a2e5b6db3458c896e68d0cfa76d04",
            "value": " 0/3 [00:00&lt;?, ?it/s]"
          }
        },
        "4406c5e163fa448e9d386eae06edd792": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "652bff37386b4e12a53e030f0ee212bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ccd428d75f7483d8d40e867b31c4740": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "374d4e46382b4dc2bc0d7391a997e40f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e060616c09d4f59a560397c1dccd527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3938a447918d446392f3118e21c55745": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e6a2e5b6db3458c896e68d0cfa76d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waqasm86/Kaggle-Dropbox-HuggingFace/blob/main/p1_colab_dropbox_huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMs8dQYkqomK",
        "outputId": "69da01a8-a4a9-4771-bc46-fba8e41a7ce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Aug 18 15:10:48 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# If needed: Runtime > Change runtime type > GPU (T4)\n",
        "!nvidia-smi\n",
        "\n",
        "# Fresh installs (quiet)\n",
        "!pip -q install \"transformers>=4.42\" \"accelerate>=0.33\" \"bitsandbytes>=0.43.0\" \\\n",
        "                 \"huggingface_hub>=0.24\" \"openai>=1.40.0\" \"dropbox>=12.0.2\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade transformers accelerate bitsandbytes sentencepiece safetensors openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0NLqp16FB-H",
        "outputId": "2dd7b4ae-2a28-4828-8a7f-134c9bf9138c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m119.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m786.5/786.5 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HF_TOKEN = \"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\"\n",
        "#DROPBOX_ACCESS_TOKEN=\"sl.u.AF5bmQnc57vpaG6t89Zc6ujRFL3QipS4Hhl6E4_7_JuKsNwyAyeehFYm_iE-i9j9K4Yn2d5j_96DKy2eDKnyMGjFu3EKIFLW52WrEzQZxA46xeMBU6Pd4L7U0wtd1t0ZcWeLXSFIzY18dxgI59uFZHUTUhy9YkiACoAOp4rt9_8Us_0IbD6XZXK4JK6NLpnaXDK576Y0PpvHR26toYYTlpHcZr_mmIQ6csSieUzZDF1shGM402uCdacPifbfzjklJmB9JOAZYM3zCSumNoPWyd6rx4YzHaOkg2yOVHCwwhD5dwQb46kJ-vYTtUdw4jhT7wi4jglIqp3p6L8rbNMQi4dgAs55j_heP-yiouyJIUoMjNnmf43kjBkaYUwOkUDqeb-4l2WWvUaTo_1l-_TNBOrjy43fyIGJRczs5_yH_-A3Oktl04YciK1EWnMkETcNFNReIlOyiH9yZgEGEn9gwScnb5mBdbpUNP9E6Y4QapqY9FYj8WN1hv15rKJ1NVUYgtYhzElx7kLhLiWte5kiGptuwlSmUEpJSt_DPto29IbbIgFsA8YRelMSmlRUnUQvFYGUhUzFHAbVuk_KhSk-eNTFt7dMzDgI98VEjh2AK_nQGQondGgd4LAOa4koQeOEjD3SrMbV_Q9hh4seL8FXbfBg74CGWQvThk9Xl0KSoAjciQ5bL3xfYb1mh2wQf2WiPSEOwitqAQRZGqFaQBWuv8GRTsqp_77s5Tl4uWBBr0Pi0gQjUvatXrOylgydFafZa4set7Fa-CTjYBKhBYZ0IYpR4e-FewDpcCqrO8_D66txBuwENa4sg77g29058S-CiJdeQlm4cOOEv4DhjT6FXRMEqiGalmtj0XIkeDRVQDKJzjTirsnq9AUpT24izL7-jyXrBLr3gc_LFCdAxzGAVTOLEEX7CWaeDIAco8IhI6wWnbjkhhJqdGzPboltCOCgpjCIc7W_h7LKeA3QMR5-ZQVXedmh70IHjaAJT7QO7hSfoom8dOFV5I5MGbGYxutEggwtQ2z3SobhrtsfTb1fVTxzdUk7i4aLHQkxQhvtin1WPr01f2d_hW3KJ59T7U_5eEzAo1M3mfW6alANPieO3zSZbRc9WVw8q3BLICkE34eFi-pwBzBNMShFs2CNYv4eP4wblXF2Y1ln3kVyGxLImAxNN6SLcL2aQJZsmgdt7jeKxkRZvrH89tzowrti0bVpx6fjXOpdZbTAInBznLrmWZARsvBAZywTB2iJP0lHzaO_Xi8s2kUb8jbDIl9fXObmqGQ\"\n"
      ],
      "metadata": {
        "id": "nbZ4PjL-qy0u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib\n",
        "\n",
        "# === REQUIRED if you want to pull weights from your Dropbox ===\n",
        "# Generate a short-lived Dropbox access token (App Console or personal token) and paste here or set via Colab \"Secrets\".\n",
        "os.environ.setdefault(\"DROPBOX_ACCESS_TOKEN\", DROPBOX_ACCESS_TOKEN)\n",
        "\n",
        "# OPTIONAL: For online inference fallback (last section)\n",
        "os.environ.setdefault(\"HF_TOKEN\", HF_TOKEN)\n",
        "\n",
        "# Your Dropbox folder that already contains the HF mirror:\n",
        "DBX_MODEL_DIR = \"/p1-gpt-oss-20b\"  # your path from Kaggle work, unchanged\n",
        "LOCAL_MODEL_DIR = \"/content/gpt-oss-20b\"  # Colab temp storage target\n",
        "pathlib.Path(LOCAL_MODEL_DIR).mkdir(parents=True, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "x8SRRi3FvVEn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"openai/gpt-oss-20b\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe(messages)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai/gpt-oss-20b\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "\ttokenize=True,\n",
        "\treturn_dict=True,\n",
        "\treturn_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=40)\n",
        "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://router.huggingface.co/v1\",\n",
        "    api_key=os.environ[\"HF_TOKEN\"],\n",
        ")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"openai/gpt-oss-20b\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the capital of France?\"\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "uzsAxMeBqrnB",
        "outputId": "abc89e9d-c5a0-4cf3-fb9e-f56d081e624c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom transformers import pipeline\\n\\npipe = pipeline(\"text-generation\", model=\"openai/gpt-oss-20b\")\\nmessages = [\\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\\n]\\npipe(messages)\\n\\n\\n\\n\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")\\nmodel = AutoModelForCausalLM.from_pretrained(\"openai/gpt-oss-20b\")\\nmessages = [\\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\\n]\\ninputs = tokenizer.apply_chat_template(\\n\\tmessages,\\n\\tadd_generation_prompt=True,\\n\\ttokenize=True,\\n\\treturn_dict=True,\\n\\treturn_tensors=\"pt\",\\n).to(model.device)\\n\\noutputs = model.generate(**inputs, max_new_tokens=40)\\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\\n\\n\\n\\n\\nimport os\\nfrom openai import OpenAI\\n\\nclient = OpenAI(\\n    base_url=\"https://router.huggingface.co/v1\",\\n    api_key=os.environ[\"HF_TOKEN\"],\\n)\\n\\ncompletion = client.chat.completions.create(\\n    model=\"openai/gpt-oss-20b\",\\n    messages=[\\n        {\\n            \"role\": \"user\",\\n            \"content\": \"What is the capital of France?\"\\n        }\\n    ],\\n)\\n\\nprint(completion.choices[0].message)\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stEJz2zlyPjZ",
        "outputId": "ecebf7b8-1313-4f43-c195-501df18c18f4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"error\": \"invalid_grant\", \"error_description\": \"code doesn't exist or has expired\"}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wi_mW9gPyPgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hfsPVONmyPXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ycUgdQxIyPU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JxeCv_G9yPMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dropbox, os, posixpath, pathlib\n",
        "from dropbox.exceptions import AuthError, ApiError\n",
        "from google.colab import userdata\n",
        "userdata.get('secretName')\n",
        "\n",
        "\n",
        "\n",
        "DBX_TOKEN = os.environ.get(\"DROPBOX_ACCESS_TOKEN\")\n",
        "assert DBX_TOKEN and DBX_TOKEN != \"PASTE_YOUR_DROPBOX_TOKEN_HERE\", \"Set DROPBOX_ACCESS_TOKEN first.\"\n",
        "\n",
        "dbx = dropbox.Dropbox(DBX_TOKEN)\n",
        "try:\n",
        "    dbx.users_get_current_account()\n",
        "except AuthError as e:\n",
        "    raise SystemExit(f\"Dropbox auth failed: {e}\")\n",
        "\n",
        "def list_folder_recursive(path=\"/\"):\n",
        "    # returns list of (is_folder, path_lower)\n",
        "    result = []\n",
        "    def _walk(p):\n",
        "        resp = dbx.files_list_folder(p, recursive=False)\n",
        "        for e in resp.entries:\n",
        "            if isinstance(e, dropbox.files.FolderMetadata):\n",
        "                result.append((True, e.path_lower))\n",
        "                _walk(e.path_lower)\n",
        "            else:\n",
        "                result.append((False, e.path_lower))\n",
        "        while resp.has_more:\n",
        "            resp = dbx.files_list_folder_continue(resp.cursor)\n",
        "            for e in resp.entries:\n",
        "                if isinstance(e, dropbox.files.FolderMetadata):\n",
        "                    result.append((True, e.path_lower))\n",
        "                    _walk(e.path_lower)\n",
        "                else:\n",
        "                    result.append((False, e.path_lower))\n",
        "    _walk(path)\n",
        "    return result\n",
        "\n",
        "def ensure_local_dirs(dbx_path, local_root):\n",
        "    rel = dbx_path[len(DBX_MODEL_DIR):].lstrip(\"/\")\n",
        "    local_path = os.path.join(local_root, rel)\n",
        "    pathlib.Path(local_path).mkdir(parents=True, exist_ok=True)\n",
        "    return local_path\n",
        "\n",
        "def download_file(dbx_path, local_root):\n",
        "    rel = dbx_path[len(DBX_MODEL_DIR):].lstrip(\"/\")\n",
        "    local_path = os.path.join(local_root, rel)\n",
        "    pathlib.Path(os.path.dirname(local_path)).mkdir(parents=True, exist_ok=True)\n",
        "    md = dbx.files_get_metadata(dbx_path)\n",
        "    if os.path.exists(local_path) and os.path.getsize(local_path) == md.size:\n",
        "        return  # already synced\n",
        "    with open(local_path, \"wb\") as f:\n",
        "        md, res = dbx.files_download(dbx_path)\n",
        "        f.write(res.content)\n",
        "\n",
        "# Walk and sync\n",
        "items = list_folder_recursive(DBX_MODEL_DIR)\n",
        "folders = [p for is_dir, p in items if is_dir]\n",
        "files   = [p for is_dir, p in items if not is_dir]\n",
        "\n",
        "for folder in folders:\n",
        "    ensure_local_dirs(folder, LOCAL_MODEL_DIR)\n",
        "\n",
        "for fpath in files:\n",
        "    download_file(fpath, LOCAL_MODEL_DIR)\n",
        "\n",
        "print(\"Sync complete. Local contents:\")\n",
        "!find /content/gpt-oss-20b -maxdepth 2 -type f | sed -n '1,200p'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eO0bEcrRva3V",
        "outputId": "e5bc156e-b6b1-43db-c209-c868fcd3af8a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:dropbox:Unable to refresh access token without                 refresh token and app key\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3712373189.py\", line 9, in <cell line: 0>\n",
            "    dbx.users_get_current_account()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/dropbox/base.py\", line 5870, in users_get_current_account\n",
            "    r = self.request(\n",
            "        ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/dropbox/dropbox_client.py\", line 326, in request\n",
            "    res = self.request_json_string_with_retry(host,\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/dropbox/dropbox_client.py\", line 476, in request_json_string_with_retry\n",
            "    return self.request_json_string(host,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/dropbox/dropbox_client.py\", line 595, in request_json_string\n",
            "    self.raise_dropbox_error_for_resp(r)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/dropbox/dropbox_client.py\", line 638, in raise_dropbox_error_for_resp\n",
            "    raise AuthError(request_id, err)\n",
            "dropbox.exceptions.AuthError: AuthError('8cca8803931b40318e55d3fbedbd9881', AuthError('expired_access_token', None))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-3712373189.py\", line 11, in <cell line: 0>\n",
            "    raise SystemExit(f\"Dropbox auth failed: {e}\")\n",
            "SystemExit: Dropbox auth failed: AuthError('8cca8803931b40318e55d3fbedbd9881', AuthError('expired_access_token', None))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1671, in getframeinfo\n",
            "    lineno = frame.f_lineno\n",
            "             ^^^^^^^^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'f_lineno'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3712373189.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdbx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musers_get_current_account\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mAuthError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dropbox/base.py\u001b[0m in \u001b[0;36musers_get_current_account\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5869\u001b[0m         \u001b[0marg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5870\u001b[0;31m         r = self.request(\n\u001b[0m\u001b[1;32m   5871\u001b[0m             \u001b[0musers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_account\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dropbox/dropbox_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, route, namespace, request_arg, request_binary, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         res = self.request_json_string_with_retry(host,\n\u001b[0m\u001b[1;32m    327\u001b[0m                                                   \u001b[0mroute_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dropbox/dropbox_client.py\u001b[0m in \u001b[0;36mrequest_json_string_with_retry\u001b[0;34m(self, host, route_name, route_style, request_json_arg, auth_type, request_binary, timeout)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m                 return self.request_json_string(host,\n\u001b[0m\u001b[1;32m    477\u001b[0m                                                 \u001b[0mroute_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dropbox/dropbox_client.py\u001b[0m in \u001b[0;36mrequest_json_string\u001b[0;34m(self, host, func_name, route_style, request_json_arg, auth_type, request_binary, timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m                                )\n\u001b[0;32m--> 595\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_dropbox_error_for_resp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m         \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x-dropbox-request-id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dropbox/dropbox_client.py\u001b[0m in \u001b[0;36mraise_dropbox_error_for_resp\u001b[0;34m(self, res)\u001b[0m\n\u001b[1;32m    637\u001b[0m                 AuthError_validator, res.json()['error'])\n\u001b[0;32m--> 638\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAuthError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mHTTP_STATUS_INVALID_PATH_ROOT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAuthError\u001b[0m: AuthError('8cca8803931b40318e55d3fbedbd9881', AuthError('expired_access_token', None))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3712373189.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mAuthError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dropbox auth failed: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: Dropbox auth failed: AuthError('8cca8803931b40318e55d3fbedbd9881', AuthError('expired_access_token', None))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3DSPAwOdva0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z4YBQW-jvax1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R4nR-3WcvavH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EfxOoYnTvasF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib\n",
        "from google.colab import userdata\n",
        "\n",
        "# ---- Dropbox (preferred: offline/refreshable) ----\n",
        "os.environ[\"DBX_APP_KEY\"]       = userdata.get('DBX_APP_KEY')\n",
        "os.environ[\"DBX_APP_SECRET\"]    = userdata.get('DBX_APP_SECRET')\n",
        "os.environ[\"DBX_REFRESH_TOKEN\"] = userdata.get('DBX_REFRESH_TOKEN')\n",
        "\n",
        "\n",
        "# ---- Hugging Face (for fallback download & router test) ----\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "\n",
        "DBX_MODEL_DIR  = \"/p1-gpt-oss-20b\"         # your Dropbox folder from Kaggle step\n",
        "LOCAL_MODEL_DIR = \"/content/gpt-oss-20b\"   # Colab temp storage\n",
        "pathlib.Path(LOCAL_MODEL_DIR).mkdir(parents=True, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "k_OB2DMFrHfW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, posixpath, zipfile, io\n",
        "import dropbox\n",
        "from dropbox.exceptions import AuthError, ApiError\n",
        "\n",
        "LOCAL_MODEL_DIR = \"/content/gpt-oss-20b\"\n",
        "DBX_MODEL_DIR   = \"/p1-gpt-oss-20b\"\n",
        "\n",
        "def get_dbx_client():\n",
        "    # Prefer refreshable client (offline access)\n",
        "    app_key       = os.environ.get(\"DBX_APP_KEY\")\n",
        "    app_secret    = os.environ.get(\"DBX_APP_SECRET\")\n",
        "    refresh_token = os.environ.get(\"DBX_REFRESH_TOKEN\")\n",
        "    if app_key and app_secret and refresh_token:\n",
        "        dbx = dropbox.Dropbox(oauth2_refresh_token=refresh_token,\n",
        "                              app_key=app_key, app_secret=app_secret)\n",
        "        dbx.users_get_current_account()  # raises if invalid\n",
        "        return dbx, \"refresh\"\n",
        "    # Fallback: short-lived access token\n",
        "    access_token = os.environ.get(\"DROPBOX_ACCESS_TOKEN\") or \"\"\n",
        "    if access_token:\n",
        "        dbx = dropbox.Dropbox(access_token)\n",
        "        dbx.users_get_current_account()\n",
        "        return dbx, \"short\"\n",
        "    return None, None\n",
        "\n",
        "def download_folder_as_zip(dbx, dbx_folder, local_dir):\n",
        "    \"\"\"Fast path: download the whole folder as a zip, then extract.\"\"\"\n",
        "    zip_path = \"/content/model.zip\"\n",
        "    # files_download_zip only works for folders, not root\n",
        "    md, res = dbx.files_download_zip(dbx_folder)\n",
        "    with open(zip_path, \"wb\") as f:\n",
        "        for chunk in res.iter_content(1024 * 1024):\n",
        "            f.write(chunk)\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        zf.extractall(local_dir)\n",
        "\n",
        "def list_folder_recursive(dbx, path):\n",
        "    result = []\n",
        "    def _walk(p):\n",
        "        resp = dbx.files_list_folder(p, recursive=False)\n",
        "        for e in resp.entries:\n",
        "            if isinstance(e, dropbox.files.FolderMetadata):\n",
        "                result.append((True, e.path_lower))\n",
        "                _walk(e.path_lower)\n",
        "            else:\n",
        "                result.append((False, e.path_lower))\n",
        "        while resp.has_more:\n",
        "            resp = dbx.files_list_folder_continue(resp.cursor)\n",
        "            for e in resp.entries:\n",
        "                if isinstance(e, dropbox.files.FolderMetadata):\n",
        "                    result.append((True, e.path_lower))\n",
        "                    _walk(e.path_lower)\n",
        "                else:\n",
        "                    result.append((False, e.path_lower))\n",
        "    _walk(path)\n",
        "    return result\n",
        "\n",
        "def ensure_local_dirs(dbx_path, local_root):\n",
        "    rel = dbx_path[len(DBX_MODEL_DIR):].lstrip(\"/\")\n",
        "    local_path = os.path.join(local_root, rel)\n",
        "    pathlib.Path(local_path).mkdir(parents=True, exist_ok=True)\n",
        "    return local_path\n",
        "\n",
        "def download_file(dbx, dbx_path, local_root):\n",
        "    rel = dbx_path[len(DBX_MODEL_DIR):].lstrip(\"/\")\n",
        "    local_path = os.path.join(local_root, rel)\n",
        "    pathlib.Path(os.path.dirname(local_path)).mkdir(parents=True, exist_ok=True)\n",
        "    md = dbx.files_get_metadata(dbx_path)\n",
        "    if os.path.exists(local_path) and os.path.getsize(local_path) == getattr(md, \"size\", None):\n",
        "        return\n",
        "    md, res = dbx.files_download(dbx_path)\n",
        "    with open(local_path, \"wb\") as f:\n",
        "        f.write(res.content)\n",
        "\n",
        "synced = False\n",
        "try:\n",
        "    dbx, mode = get_dbx_client()\n",
        "    if dbx is None:\n",
        "        raise RuntimeError(\"No valid Dropbox credentials found.\")\n",
        "\n",
        "    # Try the fast ZIP route first (if your app has permission and folder isn’t huge)\n",
        "    try:\n",
        "        download_folder_as_zip(dbx, DBX_MODEL_DIR, LOCAL_MODEL_DIR)\n",
        "        synced = True\n",
        "        print(f\"Downloaded whole folder via ZIP using {mode}-auth.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ZIP download not available/failed ({e}); falling back to per-file sync...\")\n",
        "        items = list_folder_recursive(dbx, DBX_MODEL_DIR)\n",
        "        folders = [p for is_dir, p in items if is_dir]\n",
        "        files   = [p for is_dir, p in items if not is_dir]\n",
        "        for folder in folders:\n",
        "            ensure_local_dirs(folder, LOCAL_MODEL_DIR)\n",
        "        for fpath in files:\n",
        "            download_file(dbx, fpath, LOCAL_MODEL_DIR)\n",
        "        synced = True\n",
        "        print(f\"Synced {len(files)} files via per-file method using {mode}-auth.\")\n",
        "\n",
        "except AuthError as e:\n",
        "    print(\"Dropbox authentication failed. Will try HF fallback next.\\n\", e)\n",
        "except ApiError as e:\n",
        "    print(\"Dropbox API error. Will try HF fallback next.\\n\", e)\n",
        "\n",
        "# Show what (if anything) we got\n",
        "!find /content/gpt-oss-20b -maxdepth 2 -type f | sed -n '1,200p'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "Uv78VYxczCJd",
        "outputId": "1f929e65-2047-4248-a89b-d5d43eff75c7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadInputError",
          "evalue": "BadInputError('a68a457a667c4b70b4827fe9c40f9eea', '{\"error\": \"invalid_client: Invalid client_id or client_secret\"}')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadInputError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3046148461.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0msynced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mdbx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dbx_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdbx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No valid Dropbox credentials found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3046148461.py\u001b[0m in \u001b[0;36mget_dbx_client\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         dbx = dropbox.Dropbox(oauth2_refresh_token=refresh_token,\n\u001b[1;32m     15\u001b[0m                               app_key=app_key, app_secret=app_secret)\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdbx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musers_get_current_account\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# raises if invalid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdbx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"refresh\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Fallback: short-lived access token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dropbox/base.py\u001b[0m in \u001b[0;36musers_get_current_account\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5868\u001b[0m         \"\"\"\n\u001b[1;32m   5869\u001b[0m         \u001b[0marg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5870\u001b[0;31m         r = self.request(\n\u001b[0m\u001b[1;32m   5871\u001b[0m             \u001b[0musers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_account\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5872\u001b[0m             \u001b[0;34m'users'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dropbox/dropbox_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, route, namespace, request_arg, request_binary, timeout)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \"\"\"\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_and_refresh_access_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mhost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'host'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'api'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dropbox/dropbox_client.py\u001b[0m in \u001b[0;36mcheck_and_refresh_access_token\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mneeds_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oauth2_access_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mneeds_refresh\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mneeds_token\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcan_refresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh_access_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrefresh_access_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAPI_HOST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dropbox/dropbox_client.py\u001b[0m in \u001b[0;36mrefresh_access_token\u001b[0;34m(self, host, scope)\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_dropbox_error_for_resp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mtoken_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dropbox/dropbox_client.py\u001b[0m in \u001b[0;36mraise_dropbox_error_for_resp\u001b[0;34m(self, res)\u001b[0m\n\u001b[1;32m    627\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mAuthError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mBadInputError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mBadInputError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadInputError\u001b[0m: BadInputError('a68a457a667c4b70b4827fe9c40f9eea', '{\"error\": \"invalid_client: Invalid client_id or client_secret\"}')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load & validate Dropbox creds (Colab/Kaggle friendly) ---\n",
        "import os, re\n",
        "try:\n",
        "    # Colab\n",
        "    from google.colab import userdata  # noqa\n",
        "    def _get(name):\n",
        "        v = userdata.get(name)\n",
        "        return v.strip() if isinstance(v, str) else None\n",
        "except Exception:\n",
        "    # Fallback: plain env\n",
        "    def _get(name):\n",
        "        v = os.environ.get(name)\n",
        "        return v.strip() if isinstance(v, str) else None\n",
        "\n",
        "APP_KEY       = _get(\"DBX_APP_KEY\")\n",
        "APP_SECRET    = _get(\"DBX_APP_SECRET\")\n",
        "REFRESH_TOKEN = _get(\"DBX_REFRESH_TOKEN\")\n",
        "ACCESS_TOKEN  = (_get(\"DROPBOX_ACCESS_TOKEN\") or \"\")\n",
        "\n",
        "def _mask(s, keep=4):\n",
        "    if not s: return \"∅\"\n",
        "    s = re.sub(r\"\\s+\", \"\", s)\n",
        "    return (\"*\"*(max(0,len(s)-keep))) + s[-keep:]\n",
        "\n",
        "print(\"DBG app_key:\", _mask(APP_KEY))\n",
        "print(\"DBG app_secret:\", _mask(APP_SECRET))\n",
        "print(\"DBG refresh_token:\", _mask(REFRESH_TOKEN))\n",
        "print(\"DBG access_token:\", _mask(ACCESS_TOKEN))\n",
        "\n",
        "assert APP_KEY and APP_SECRET and REFRESH_TOKEN, \\\n",
        "    \"Missing app key/secret/refresh token. Set DBX_APP_KEY/DBX_APP_SECRET/DBX_REFRESH_TOKEN in Colab secrets.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLiZ-Wf36RNo",
        "outputId": "1d2d3ff8-872e-4831-db4d-8b6eef170fe4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DBG app_key: ***********azmz\n",
            "DBG app_secret: ***********802s\n",
            "DBG refresh_token: ************************************************************I6EE\n",
            "DBG access_token: ************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0wrM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i9asf4Jg6RKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kDXcm6ZB6RFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jM2r3NUL6RCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oaBmphof6Q5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dropbox import Dropbox\n",
        "\n",
        "# Replace these with YOUR actual credentials\n",
        "app_key       = \"b6r892qeer2azmz\"\n",
        "app_secret    = \"ow5tme8mnfx802s\"\n",
        "refresh_token = \"1I5t8XAbWgcAAAAAAAAAAQlxv3j5zgDJKkGaaSJD09RdLs5rNUX4BlmDa38-I6EE\"\n",
        "\n",
        "# Initialize Dropbox with refresh token\n",
        "dbx = Dropbox(\n",
        "    oauth2_refresh_token=refresh_token,\n",
        "    app_key=app_key,\n",
        "    app_secret=app_secret\n",
        ")\n",
        "\n",
        "# Test connection\n",
        "try:\n",
        "    print(dbx.users_get_current_account())  # Should return your account info\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm_ncv0qzFLS",
        "outputId": "ce28fa28-6c91-4a63-fa31-f4b36443d144"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FullAccount(account_id='dbid:AABp6OXGkbpaIZBfrvHfyPkuH9q2F6EJ4lU', account_type=AccountType('pro', None), country='CA', disabled=False, email='waqasm86@gmail.com', email_verified=True, is_paired=False, locale='en', name=Name(abbreviated_name='MW', display_name='M Waqas', familiar_name='M', given_name='M', surname='Waqas'), profile_photo_url=NOT_SET, referral_link='https://www.dropbox.com/referrals/AAAgmZFJ3sdf-qcvwbb-EU2SgKt6k84JXmA?src=app9-5059235', root_info=UserRootInfo(home_namespace_id='208040503', root_namespace_id='208040503'), team=NOT_SET, team_member_id=NOT_SET)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sSWXSFCrCoQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D1gfHb8SCoOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dxUARyLmCoLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MKldLhBaCoIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YiVugR2QCoFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KLnr8dejCn9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Dropbox sync (Colab/Kaggle friendly) ---\n",
        "import os, re, io, zipfile, pathlib, posixpath, typing as T\n",
        "import dropbox\n",
        "from dropbox.exceptions import AuthError, ApiError, BadInputError\n",
        "\n",
        "# -------- Secrets resolver (works in Colab or plain env) --------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    def _get(name: str) -> str | None:\n",
        "        v = userdata.get(name)\n",
        "        return v.strip() if isinstance(v, str) else None\n",
        "except Exception:\n",
        "    def _get(name: str) -> str | None:\n",
        "        v = os.environ.get(name)\n",
        "        return v.strip() if isinstance(v, str) else None\n",
        "\n",
        "def _mask(s: str | None, keep: int = 4) -> str:\n",
        "    if not s: return \"∅\"\n",
        "    s = re.sub(r\"\\s+\", \"\", s)\n",
        "    return (\"*\" * max(0, len(s) - keep)) + s[-keep:]\n",
        "\n",
        "APP_KEY       = _get(\"DBX_APP_KEY\")\n",
        "APP_SECRET    = _get(\"DBX_APP_SECRET\")\n",
        "REFRESH_TOKEN = _get(\"DBX_REFRESH_TOKEN\")\n",
        "ACCESS_TOKEN  = _get(\"DROPBOX_ACCESS_TOKEN\") or \"\"\n",
        "\n",
        "print(\"[dbg] app_key:       \", _mask(APP_KEY))\n",
        "print(\"[dbg] app_secret:    \", _mask(APP_SECRET))\n",
        "print(\"[dbg] refresh_token: \", _mask(REFRESH_TOKEN))\n",
        "print(\"[dbg] access_token:  \", _mask(ACCESS_TOKEN))\n",
        "\n",
        "DBX_MODEL_DIR   = \"/p1-gpt-oss-20b\"      # must start with \"/\"\n",
        "LOCAL_MODEL_DIR = \"/content/gpt-oss-20b\" # make sure it exists\n",
        "pathlib.Path(LOCAL_MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -------- Auth helper: prefer refresh, else access token --------\n",
        "def get_dbx_client():\n",
        "    # Try refreshable client first (requires matching app key/secret & refresh token)\n",
        "    if APP_KEY and APP_SECRET and REFRESH_TOKEN:\n",
        "        try:\n",
        "            dbx = dropbox.Dropbox(\n",
        "                oauth2_refresh_token=REFRESH_TOKEN,\n",
        "                app_key=APP_KEY,\n",
        "                app_secret=APP_SECRET,\n",
        "            )\n",
        "            dbx.users_get_current_account()\n",
        "            return dbx, \"refresh\"\n",
        "        except BadInputError as e:\n",
        "            # Most common: app_key/app_secret mismatch with the refresh token (\"invalid_client\")\n",
        "            print(\"[warn] Refresh-token auth failed (invalid_client). Will try access token next.\\n \", e)\n",
        "        except AuthError as e:\n",
        "            print(\"[warn] Refresh-token auth failed (AuthError). Will try access token next.\\n \", e)\n",
        "\n",
        "    # Fallback: short-lived access token (works fine for a single session)\n",
        "    if ACCESS_TOKEN:\n",
        "        try:\n",
        "            dbx = dropbox.Dropbox(oauth2_access_token=ACCESS_TOKEN)\n",
        "            dbx.users_get_current_account()\n",
        "            return dbx, \"access\"\n",
        "        except AuthError as e:\n",
        "            print(\"[warn] Access-token auth failed.\\n \", e)\n",
        "\n",
        "    return None, None\n",
        "\n",
        "# -------- Utilities --------\n",
        "def download_folder_as_zip(dbx: dropbox.Dropbox, dbx_folder: str, local_dir: str):\n",
        "    \"\"\"\n",
        "    Fast path: download the whole folder as a zip and extract.\n",
        "    Limitations: very large folders may fail; requires content permission.\n",
        "    \"\"\"\n",
        "    zip_path = \"/content/model.zip\"\n",
        "    # files_download_zip returns (metadata, HTTPResponse)\n",
        "    md, http_resp = dbx.files_download_zip(dbx_folder)\n",
        "    with open(zip_path, \"wb\") as f:\n",
        "        for chunk in http_resp.iter_content(1024 * 1024):\n",
        "            f.write(chunk)\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        zf.extractall(local_dir)\n",
        "\n",
        "def list_folder_recursive(dbx: dropbox.Dropbox, path: str) -> list[tuple[bool, str]]:\n",
        "    \"\"\"\n",
        "    Recursively list folder; returns [(is_dir, path_lower), ...]\n",
        "    \"\"\"\n",
        "    result: list[tuple[bool, str]] = []\n",
        "    def _walk(p: str):\n",
        "        resp = dbx.files_list_folder(p, recursive=False)\n",
        "        while True:\n",
        "            for e in resp.entries:\n",
        "                if isinstance(e, dropbox.files.FolderMetadata):\n",
        "                    result.append((True, e.path_lower))\n",
        "                    _walk(e.path_lower)\n",
        "                else:\n",
        "                    result.append((False, e.path_lower))\n",
        "            if not resp.has_more:\n",
        "                break\n",
        "            resp = dbx.files_list_folder_continue(resp.cursor)\n",
        "    _walk(path)\n",
        "    return result\n",
        "\n",
        "def ensure_local_dirs(dbx_path: str, local_root: str) -> str:\n",
        "    rel = dbx_path[len(DBX_MODEL_DIR):].lstrip(\"/\")\n",
        "    local_path = os.path.join(local_root, rel)\n",
        "    pathlib.Path(local_path).mkdir(parents=True, exist_ok=True)\n",
        "    return local_path\n",
        "\n",
        "def download_file(dbx: dropbox.Dropbox, dbx_path: str, local_root: str):\n",
        "    rel = dbx_path[len(DBX_MODEL_DIR):].lstrip(\"/\")\n",
        "    local_path = os.path.join(local_root, rel)\n",
        "    pathlib.Path(os.path.dirname(local_path)).mkdir(parents=True, exist_ok=True)\n",
        "    md = dbx.files_get_metadata(dbx_path)\n",
        "    size = getattr(md, \"size\", None)\n",
        "    if size is not None and os.path.exists(local_path) and os.path.getsize(local_path) == size:\n",
        "        return\n",
        "    md, res = dbx.files_download(dbx_path)\n",
        "    with open(local_path, \"wb\") as f:\n",
        "        f.write(res.content)\n",
        "\n",
        "# -------- Run sync --------\n",
        "synced = False\n",
        "dbx, mode = get_dbx_client()\n",
        "if not dbx:\n",
        "    raise RuntimeError(\"No valid Dropbox credentials (refresh or access token) could authenticate.\")\n",
        "\n",
        "print(f\"[info] Auth mode: {mode}\")\n",
        "\n",
        "try:\n",
        "    # Validate the model folder exists before attempting ZIP\n",
        "    try:\n",
        "        dbx.files_get_metadata(DBX_MODEL_DIR)\n",
        "    except ApiError as e:\n",
        "        raise RuntimeError(f\"Dropbox folder not found or no access: {DBX_MODEL_DIR}\\n{e}\")\n",
        "\n",
        "    try:\n",
        "        download_folder_as_zip(dbx, DBX_MODEL_DIR, LOCAL_MODEL_DIR)\n",
        "        synced = True\n",
        "        print(f\"[info] Downloaded whole folder as ZIP using {mode}-auth.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] ZIP download not available/failed ({e}); falling back to per-file sync...\")\n",
        "        items = list_folder_recursive(dbx, DBX_MODEL_DIR)\n",
        "        folders = [p for is_dir, p in items if is_dir]\n",
        "        files   = [p for is_dir, p in items if not is_dir]\n",
        "        for folder in folders:\n",
        "            ensure_local_dirs(folder, LOCAL_MODEL_DIR)\n",
        "        for fpath in files:\n",
        "            download_file(dbx, fpath, LOCAL_MODEL_DIR)\n",
        "        synced = True\n",
        "        print(f\"[info] Synced {len(files)} files via per-file method using {mode}-auth.\")\n",
        "except AuthError as e:\n",
        "    print(\"[error] Dropbox auth failed during operations.\", e)\n",
        "except ApiError as e:\n",
        "    print(\"[error] Dropbox API error during operations.\", e)\n",
        "\n",
        "# -------- Show a quick sample of what landed --------\n",
        "import subprocess, shlex\n",
        "print(\"\\n[tree] First few files in local mirror:\")\n",
        "subprocess.run(shlex.split(\"bash -lc 'find /content/gpt-oss-20b -type f | head -n 50'\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJkosCog6e5v",
        "outputId": "bbcaa364-f26a-42d4-8649-cdf780263bc7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[dbg] app_key:        ***********azmz\n",
            "[dbg] app_secret:     ***********802s\n",
            "[dbg] refresh_token:  ************************************************************I6EE\n",
            "[dbg] access_token:   ************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0wrM\n",
            "[info] Auth mode: refresh\n",
            "[info] Downloaded whole folder as ZIP using refresh-auth.\n",
            "\n",
            "[tree] First few files in local mirror:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['bash', '-lc', 'find /content/gpt-oss-20b -type f | head -n 50'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from google.colab import userdata\n",
        "for k in [\"DBX_APP_KEY\",\"DBX_APP_SECRET\",\"DBX_REFRESH_TOKEN\",\"DROPBOX_ACCESS_TOKEN\"]:\n",
        "    v = userdata.get(k)\n",
        "    if v: os.environ[k] = v.strip()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "id": "pA4DHq8JCozI",
        "outputId": "6a50174d-7b9a-4f17-8ae0-800d4a63ee21"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom google.colab import userdata\\nfor k in [\"DBX_APP_KEY\",\"DBX_APP_SECRET\",\"DBX_REFRESH_TOKEN\",\"DROPBOX_ACCESS_TOKEN\"]:\\n    v = userdata.get(k)\\n    if v: os.environ[k] = v.strip()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TrQkYxapEK3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6HQBQoFHFQqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UokBymInFQlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IyKlFMelFQhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, torch, gc\n",
        "from typing import Optional\n",
        "\n",
        "# --- Secrets: use Colab's store if present, else env ---\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    def _get(k: str) -> Optional[str]:\n",
        "        v = userdata.get(k)\n",
        "        return v.strip() if isinstance(v, str) else None\n",
        "except Exception:\n",
        "    def _get(k: str) -> Optional[str]:\n",
        "        v = os.environ.get(k)\n",
        "        return v.strip() if isinstance(v, str) else None\n",
        "\n",
        "HF_TOKEN = _get(\"HF_TOKEN\")\n",
        "if HF_TOKEN:\n",
        "    os.environ[\"HF_TOKEN\"] = HF_TOKEN  # for HF router (sample 3)\n",
        "\n",
        "LOCAL_MODEL_DIR = \"/content/gpt-oss-20b\"  # already synced from Dropbox\n",
        "\n",
        "# --- Common HF kwargs (4-bit to fit on a single T4) ---\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "load_kwargs = dict(\n",
        "    trust_remote_code=True,           # gpt-oss-20b has custom chat template\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"auto\",                # let Accelerate place layers\n",
        "    load_in_4bit=True,               # quantize on load (fits T4)\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    attn_implementation=\"eager\",     # T4-safe\n",
        ")\n",
        "\n",
        "def build_chat_inputs(tok, messages):\n",
        "    return tok.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_dict=True,\n",
        "    )\n",
        "\n",
        "def try_local_pipeline():\n",
        "    from transformers import pipeline\n",
        "    tok = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIR, trust_remote_code=True)\n",
        "    gen = pipeline(\n",
        "        task=\"text-generation\",\n",
        "        model=LOCAL_MODEL_DIR,\n",
        "        tokenizer=tok,\n",
        "        **load_kwargs\n",
        "    )\n",
        "    # For chat models, pass a chat-formatted prompt\n",
        "    prompt = tok.apply_chat_template(\n",
        "        [{\"role\":\"user\",\"content\":\"Who are you?\"}],\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=False,\n",
        "    )\n",
        "    out = gen(prompt, max_new_tokens=64, do_sample=True, temperature=0.7, return_full_text=False)\n",
        "    print(\"\\n[Pipeline output]\")\n",
        "    print(out[0][\"generated_text\"].strip())\n",
        "\n",
        "def try_local_automodel():\n",
        "    tok = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIR, trust_remote_code=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(LOCAL_MODEL_DIR, **load_kwargs)\n",
        "    messages = [{\"role\":\"user\",\"content\":\"Who are you?\"}]\n",
        "    inputs = build_chat_inputs(tok, messages).to(model.device)\n",
        "    with torch.inference_mode():\n",
        "        out = model.generate(**inputs, max_new_tokens=64, do_sample=True, temperature=0.7)\n",
        "    # print only the newly generated tokens\n",
        "    generated = out[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    print(\"\\n[AutoModel output]\")\n",
        "    print(tok.decode(generated, skip_special_tokens=True).strip())\n",
        "    # free up memory for the router demo\n",
        "    del model; gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "def run_hf_router():\n",
        "    print(\"\\n[HF Router via OpenAI client]\")\n",
        "    if not HF_TOKEN:\n",
        "        raise RuntimeError(\"HF_TOKEN is not set in Colab secrets or env.\")\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(\n",
        "        base_url=\"https://router.huggingface.co/v1\",\n",
        "        api_key=HF_TOKEN,\n",
        "    )\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"openai/gpt-oss-20b\",\n",
        "        messages=[{\"role\":\"user\",\"content\":\"What is the capital of France?\"}],\n",
        "        temperature=0.7,\n",
        "        max_tokens=64,\n",
        "    )\n",
        "    print(completion.choices[0].message)\n",
        "\n",
        "# ---- Run samples with graceful fallback ----\n",
        "local_ok = True\n",
        "try:\n",
        "    try_local_pipeline()\n",
        "except Exception as e:\n",
        "    local_ok = False\n",
        "    print(f\"[warn] Local pipeline failed: {e}\")\n",
        "\n",
        "if local_ok:\n",
        "    try:\n",
        "        try_local_automodel()\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] Local AutoModel failed: {e}\")\n",
        "\n",
        "# Always show router demo (works even if local fails)\n",
        "try:\n",
        "    run_hf_router()\n",
        "except Exception as e:\n",
        "    print(f\"[warn] HF Router demo failed: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIpZ4t63FQem",
        "outputId": "86964c49-1163-40d6-e1b5-d6789013ca46"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[warn] Local pipeline failed: Unrecognized model in /content/gpt-oss-20b. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, doge, donut-swin, dots1, dpr, dpt, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
            "\n",
            "[HF Router via OpenAI client]\n",
            "ChatCompletionMessage(content='The capital of France is Paris.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning_content=\"We need to answer the question: capital of France is Paris. And we should comply with policy. It's straightforward.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YvpzkEpWHBCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install \"transformers>=4.44.2\" \"accelerate>=0.33.0\" \"bitsandbytes>=0.43.3\" \"einops>=0.7.0\" \"safetensors>=0.4.3\"\n",
        "import torch, os, json, pathlib, gc\n",
        "print(\"PyTorch:\", torch.__version__, \"CUDA available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBYieZtYHA_X",
        "outputId": "afd87592-33ad-4fd1-c452-2352d42b3b5c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.6.0+cu124 CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    def _get(k: str) -> Optional[str]:\n",
        "        v = userdata.get(k)\n",
        "        return v.strip() if isinstance(v, str) else None\n",
        "except Exception:\n",
        "    def _get(k: str) -> Optional[str]:\n",
        "        v = os.environ.get(k)\n",
        "        return v.strip() if isinstance(v, str) else None\n",
        "\n",
        "HF_TOKEN = _get(\"HF_TOKEN\")\n",
        "if HF_TOKEN:\n",
        "    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n"
      ],
      "metadata": {
        "id": "PpaE6xpkHA8Z"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, pathlib\n",
        "\n",
        "ROOT = pathlib.Path(\"/content/gpt-oss-20b\")  # your sync target\n",
        "assert ROOT.exists(), f\"{ROOT} does not exist. Mount/copy your Dropbox content there or change this path.\"\n",
        "\n",
        "CANDIDATE_FILES = {\"config.json\", \"tokenizer_config.json\"}\n",
        "REQUIRED_KEYS_IN_CONFIG = {\"model_type\"}  # should be \"gpt_oss\" for this model\n",
        "\n",
        "def is_valid_model_dir(p: pathlib.Path) -> bool:\n",
        "    cfg = p / \"config.json\"\n",
        "    if not cfg.exists():\n",
        "        return False\n",
        "    try:\n",
        "        j = json.loads(cfg.read_text())\n",
        "    except Exception:\n",
        "        return False\n",
        "    # minimal sanity\n",
        "    if not REQUIRED_KEYS_IN_CONFIG.issubset(j.keys()):\n",
        "        return False\n",
        "    # tokenizer files usually present (one of these typically exists)\n",
        "    has_tok = any((p / name).exists() for name in [\"tokenizer.json\",\"tokenizer.model\",\"vocab.json\"])\n",
        "    return has_tok\n",
        "\n",
        "def find_model_dir(root: pathlib.Path) -> Optional[pathlib.Path]:\n",
        "    # 1) try root\n",
        "    if is_valid_model_dir(root):\n",
        "        return root\n",
        "    # 2) search one level deep\n",
        "    for child in root.iterdir():\n",
        "        if child.is_dir() and is_valid_model_dir(child):\n",
        "            return child\n",
        "    # 3) search repo-like layouts (e.g., snapshot dirs)\n",
        "    for p in root.rglob(\"config.json\"):\n",
        "        if is_valid_model_dir(p.parent):\n",
        "            return p.parent\n",
        "    return None\n",
        "\n",
        "MODEL_DIR = find_model_dir(ROOT)\n",
        "print(\"Resolved MODEL_DIR:\", MODEL_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWQRUyYaHA5Y",
        "outputId": "5c13928e-a0f0-43a6-ce00-70d7977d46d6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resolved MODEL_DIR: /content/gpt-oss-20b/p1-gpt-oss-20b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "if MODEL_DIR is None:\n",
        "    target = \"/content/hf_openai_gpt-oss-20b\"\n",
        "    MODEL_DIR = snapshot_download(\n",
        "        repo_id=\"openai/gpt-oss-20b\",\n",
        "        local_dir=target,\n",
        "        local_dir_use_symlinks=False,\n",
        "        token=HF_TOKEN,  # or None if public\n",
        "        allow_patterns=[\"*.json\",\"*.safetensors\",\"*.model\",\"*.txt\",\"*.py\",\"*.md\",\"tokenizer*\",\"generation_config.json\"],\n",
        "        tqdm_class=None\n",
        "    )\n",
        "    MODEL_DIR = pathlib.Path(MODEL_DIR)\n",
        "    print(\"Downloaded to:\", MODEL_DIR)\n"
      ],
      "metadata": {
        "id": "s_TvipXKHA2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "def build_chat_inputs(tok, messages):\n",
        "    return tok.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_dict=True,\n",
        "    )\n",
        "\n",
        "print(\"Using MODEL_DIR:\", MODEL_DIR)\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=True)\n",
        "\n",
        "# ---- Pipeline path\n",
        "from transformers import pipeline\n",
        "gen = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=str(MODEL_DIR),\n",
        "    tokenizer=tok,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_cfg,\n",
        "    low_cpu_mem_usage=True,\n",
        "    attn_implementation=\"eager\",   # T4-safe\n",
        ")\n",
        "prompt = tok.apply_chat_template(\n",
        "    [{\"role\":\"user\",\"content\":\"Who are you?\"}],\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=False,\n",
        ")\n",
        "out = gen(prompt, max_new_tokens=64, do_sample=True, temperature=0.7, return_full_text=False)\n",
        "print(\"\\n[Pipeline output]\\n\", out[0][\"generated_text\"].strip())\n",
        "\n",
        "# ---- AutoModel path\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_DIR,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_cfg,\n",
        "    low_cpu_mem_usage=True,\n",
        "    attn_implementation=\"eager\",\n",
        ")\n",
        "messages = [{\"role\":\"user\",\"content\":\"Who are you?\"}]\n",
        "inputs = build_chat_inputs(tok, messages).to(model.device)\n",
        "with torch.inference_mode():\n",
        "    out_ids = model.generate(**inputs, max_new_tokens=64, do_sample=True, temperature=0.7)\n",
        "gen_ids_only = out_ids[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "print(\"\\n[AutoModel output]\\n\", tok.decode(gen_ids_only, skip_special_tokens=True).strip())\n",
        "\n",
        "# cleanup\n",
        "del model; gc.collect(); torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fbb0c2edbd284a7ca3eb17bb7e7174d5",
            "2c996db176f646cf94451041d732efee",
            "6a7ee0ac8b314bcb83bce2c29f1e0c7e",
            "d91027a9e7404f9d8a14363e01228c0f",
            "de0d8c82a6914fc89a786ca725ad5790",
            "497af52380c4490d999e34c898bb890d",
            "311e5183e773481f87aada22e6eac5f4",
            "b68698c2399a45c681157aa8e6e57263",
            "9b622b7089c04de38f3655c5a65e39de",
            "80af1edcde444a3f9a0dc6e78a1cc869",
            "b136d62ef42640ae8f4f4864a33da513",
            "6639bab5ee914950b5e7fe6f9b9077af",
            "9f866aea43c444ba92c28dacf9742808",
            "624c3c36f87547d1b60a93cd22a5561d",
            "2e2e91fea0bd4d42954cea897c4f8588",
            "e54cb87e450542f380bac337434609dd",
            "ee9342dad9d0475eb79472d18e4f7d21",
            "1645fc514a734683bea4ec848b02361f",
            "3a94c12e0f824e28b6bc1301600774c4",
            "921fad38a0ce4984afc2c46e5e7b888d",
            "d82703da76b54a409745cbfe3cfcc1fb",
            "064c0ce1e2de4bdda9aae7e3980263d9",
            "558ecd2df3834effa3a0cd260048f5b5",
            "b3a077c7486d4a5796301e7cd32a2f96",
            "74ed77885b1444f1bbe230f9847c08a5",
            "0f145c96fa314fdcadd73f0ab32ad02d",
            "654e655bcff84fd18e819ee14fc3ff15",
            "ee924c9a28c84d9a8cfe18bc27fee58e",
            "0b1ff72178454a31b860ae6f1c67fb0e",
            "7ea536aac2d34919b4b1533239b6d50b",
            "5d00ca1e85fa477a9b68bdf9192c8710",
            "c28d51be305d44eb96f8bd36bfa1b74b",
            "063d421f811146d2b7d0abc4a2b1ee54",
            "ae4abb107b93494b9cfb7b88c516bdc7",
            "43548a760dc940c3ae2d3845950f2640",
            "733a600797be4934a2d4c69325800825",
            "42fae67ae3764d039026592d0abc0941",
            "4406c5e163fa448e9d386eae06edd792",
            "652bff37386b4e12a53e030f0ee212bd",
            "8ccd428d75f7483d8d40e867b31c4740",
            "374d4e46382b4dc2bc0d7391a997e40f",
            "4e060616c09d4f59a560397c1dccd527",
            "3938a447918d446392f3118e21c55745",
            "9e6a2e5b6db3458c896e68d0cfa76d04"
          ]
        },
        "id": "Eb4YLZlUFWDJ",
        "outputId": "63b6d687-5233-4c4a-ed59-6d90de7cd2ae"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using MODEL_DIR: /content/gpt-oss-20b/p1-gpt-oss-20b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MXFP4 quantization requires triton >= 3.4.0 and kernels installed, we will default to dequantizing the model to bf16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fbb0c2edbd284a7ca3eb17bb7e7174d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6639bab5ee914950b5e7fe6f9b9077af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "558ecd2df3834effa3a0cd260048f5b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae4abb107b93494b9cfb7b88c516bdc7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Could not load model /content/gpt-oss-20b/p1-gpt-oss-20b with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>, <class 'transformers.models.gpt_oss.modeling_gpt_oss.GptOssForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 317, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 5069, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 5532, in _load_pretrained_model\n    _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)\n                                                         ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 975, in load_shard_file\n    disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 883, in _load_state_dict_into_meta_model\n    hf_quantizer.create_quantized_param(\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_mxfp4.py\", line 247, in create_quantized_param\n    dequantize(module, param_name, param_value, target_device, dq_param_name, **shard_kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/mxfp4.py\", line 342, in dequantize\n    dequantized = convert_moe_packed_tensors(getattr(module, blocks_attr), getattr(module, scales_attr))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/mxfp4.py\", line 124, in convert_moe_packed_tensors\n    idx_hi = (blk >> 4).to(torch.long)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.98 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.97 GiB is free. Process 60948 has 12.77 GiB memory in use. Of the allocated memory 10.15 GiB is allocated by PyTorch, and 2.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 317, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 5069, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 5532, in _load_pretrained_model\n    _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)\n                                                         ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 975, in load_shard_file\n    disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 883, in _load_state_dict_into_meta_model\n    hf_quantizer.create_quantized_param(\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_mxfp4.py\", line 247, in create_quantized_param\n    dequantize(module, param_name, param_value, target_device, dq_param_name, **shard_kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/mxfp4.py\", line 342, in dequantize\n    dequantized = convert_moe_packed_tensors(getattr(module, blocks_attr), getattr(module, scales_attr))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/mxfp4.py\", line 123, in convert_moe_packed_tensors\n    idx_lo = (blk & 0x0F).to(torch.long)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.98 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.97 GiB is free. Process 60948 has 12.77 GiB memory in use. Of the allocated memory 11.45 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 603, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.gpt_oss.configuration_gpt_oss.GptOssConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, MistralConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 603, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.gpt_oss.configuration_gpt_oss.GptOssConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, MistralConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n\nwhile loading with GptOssForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 317, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 5069, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 5532, in _load_pretrained_model\n    _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)\n                                                         ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 975, in load_shard_file\n    disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 883, in _load_state_dict_into_meta_model\n    hf_quantizer.create_quantized_param(\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_mxfp4.py\", line 247, in create_quantized_param\n    dequantize(module, param_name, param_value, target_device, dq_param_name, **shard_kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/mxfp4.py\", line 342, in dequantize\n    dequantized = convert_moe_packed_tensors(getattr(module, blocks_attr), getattr(module, scales_attr))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/mxfp4.py\", line 124, in convert_moe_packed_tensors\n    idx_hi = (blk >> 4).to(torch.long)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.98 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.97 GiB is free. Process 60948 has 12.77 GiB memory in use. Of the allocated memory 10.65 GiB is allocated by PyTorch, and 2.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 317, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 5069, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 5532, in _load_pretrained_model\n    _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)\n                                                         ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 975, in load_shard_file\n    disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 883, in _load_state_dict_into_meta_model\n    hf_quantizer.create_quantized_param(\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_mxfp4.py\", line 247, in create_quantized_param\n    dequantize(module, param_name, param_value, target_device, dq_param_name, **shard_kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/mxfp4.py\", line 342, in dequantize\n    dequantized = convert_moe_packed_tensors(getattr(module, blocks_attr), getattr(module, scales_attr))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/mxfp4.py\", line 124, in convert_moe_packed_tensors\n    idx_hi = (blk >> 4).to(torch.long)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.98 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.97 GiB is free. Process 60948 has 12.77 GiB memory in use. Of the allocated memory 7.79 GiB is allocated by PyTorch, and 4.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3188460887.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# ---- Pipeline path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m gen = pipeline(\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0mmodel_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m         framework, model = infer_framework_load_model(\n\u001b[0m\u001b[1;32m   1009\u001b[0m             \u001b[0madapter_path\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0madapter_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0mmodel_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_traceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0merror\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"while loading with {class_name}, an error is thrown:\\n{trace}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;34mf\"Could not load model {model} with any of the following classes: {class_tuple}. See the original errors:\\n\\n{error}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Could not load model /content/gpt-oss-20b/p1-gpt-oss-20b with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>, <class 'transformers.models.gpt_oss.modeling_gpt_oss.GptOssForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 317, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 5069, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 5532, in _load_pretrained_model\n    _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)\n                                                         ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 975, in load_shard_file\n    disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 883, in _load_state_dict_into_meta_model\n    hf_quantizer.create_quantized_param(\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_mxfp4.py\", line 247, in create_quantized_param\n    dequantize(module, param_name, param_value, target_device, dq_param_name, **shard_kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/mxfp4.py\", line 342, in dequantize\n    dequantized = convert_moe_packed_tensors(getattr(module, blocks_attr), getattr(module, scales_attr))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/mxfp4.py\", line 124, in convert_moe_packed_tensors\n    idx_hi = (blk >> 4).to(torch.long)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.98 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.97 GiB is free. Process 60948 has 12.77 GiB memory in use. Of the allocated memory 10.15 GiB is allocated by PyTorch, and 2.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 317, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 5069, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 5532, in _load_pretrained_model\n    _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)\n                                                         ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 975, in load_shard_file\n    disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 883, in _load_state_dict_into_meta_model\n    hf_quantizer.create_quantized_param(\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_mxfp4.py\", line 247, in create_quantized_param\n    dequantize(module, param_name, param_value, target_device, dq_param_name, **shard_kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/mxfp4.py\", line 342, in dequantize\n    dequantized = convert_moe_packed_tensors(getattr(module, blocks_attr), getattr(module, scales_attr))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/mxfp4.py\", line 123, in convert_moe_packed_tensors\n    idx_lo = (blk & 0x0F).to(torch.long)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.98 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.97 GiB is free. Process 60948 has 12.77 GiB memory in use. Of the allocated memory 11.45 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 603, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.gpt_oss.configuration_gpt_oss.GptOssConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, MistralConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 603, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.gpt_oss.configuration_gpt_oss.GptOssConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, MistralConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n\nwhile loading with GptOssForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 317, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 5069, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 5532, in _load_pretrained_model\n    _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)\n                                                         ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 975, in load_shard_file\n    disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 883, in _load_state_dict_into_meta_model\n    hf_quantizer.create_quantized_param(\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_mxfp4.py\", line 247, in create_quantized_param\n    dequantize(module, param_name, param_value, target_device, dq_param_name, **shard_kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/mxfp4.py\", line 342, in dequantize\n    dequantized = convert_moe_packed_tensors(getattr(module, blocks_attr), getattr(module, scales_attr))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/mxfp4.py\", line 124, in convert_moe_packed_tensors\n    idx_hi = (blk >> 4).to(torch.long)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.98 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.97 GiB is free. Process 60948 has 12.77 GiB memory in use. Of the allocated memory 10.65 GiB is allocated by PyTorch, and 2.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 317, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 5069, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 5532, in _load_pretrained_model\n    _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)\n                                                         ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 975, in load_shard_file\n    disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 883, in _load_state_dict_into_meta_model\n    hf_quantizer.create_quantized_param(\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_mxfp4.py\", line 247, in create_quantized_param\n    dequantize(module, param_name, param_value, target_device, dq_param_name, **shard_kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/mxfp4.py\", line 342, in dequantize\n    dequantized = convert_moe_packed_tensors(getattr(module, blocks_attr), getattr(module, scales_attr))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/mxfp4.py\", line 124, in convert_moe_packed_tensors\n    idx_hi = (blk >> 4).to(torch.long)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.98 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.97 GiB is free. Process 60948 has 12.77 GiB memory in use. Of the allocated memory 7.79 GiB is allocated by PyTorch, and 4.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oTdoMPsWHVMs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}