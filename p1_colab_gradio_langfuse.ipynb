{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP1BoXQzWCQk7jp19NexaF6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waqasm86/Kaggle-Dropbox-HuggingFace/blob/main/p1_colab_gradio_langfuse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -q -y transformers datasets accelerate > /dev/null\n",
        "!pip install -q transformers datasets accelerate --upgrade > /dev/null\n",
        "import transformers, datasets, accelerate\n"
      ],
      "metadata": {
        "id": "zQ0bpLTHXvjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BesZ1KYBXl2b",
        "outputId": "f35a5838-7334-4090-99b7-55a2c9cb990c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers version: 4.55.4\n",
            "accelerate version: 1.10.1\n",
            "datasets version: 4.0.0\n"
          ]
        }
      ],
      "source": [
        "print(f\"transformers version: {transformers.__version__}\")\n",
        "print(f\"accelerate version: {accelerate.__version__}\")\n",
        "print(f\"datasets version: {datasets.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install required packages\n",
        "!pip install -qU gradio langfuse plotly sentencepiece protobuf --upgrade > /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFQgornoX2JZ",
        "outputId": "0caa894f-b1c3-4c7b-c423-9ab6f07f2840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.32.0 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.32.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ozKK_32KZ0XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lAL6iVYpZ1S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f64RIm0jZ1QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import gradio as gr\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "import langfuse\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Initialize Langfuse (replace with your actual credentials)\n",
        "langfuse_client = langfuse.Langfuse(\n",
        "    public_key=\"pk-lf-12345678-1234-1234-1234-123456789012\",\n",
        "    secret_key=\"sk-lf-12345678-1234-1234-1234-123456789012\",\n",
        "    host=\"https://cloud.langfuse.com\"\n",
        ")\n",
        "\n",
        "# Load a lightweight model that can run on T4 GPU\n",
        "MODEL_NAME = \"microsoft/DialoGPT-small\"  # Very lightweight model\n",
        "\n",
        "class LightweightLLM:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "        self.trace = None\n",
        "\n",
        "    def generate_response(self, prompt: str, max_length: int = 100):\n",
        "        \"\"\"Generate response with Langfuse observability\"\"\"\n",
        "        # Start a new trace\n",
        "        trace = langfuse_client.trace(\n",
        "            name=\"llm-generation\",\n",
        "            user_id=\"demo-user\",\n",
        "            metadata={\"model\": MODEL_NAME, \"device\": str(self.device)}\n",
        "        )\n",
        "\n",
        "        generation = trace.generation(\n",
        "            name=\"chat-completion\",\n",
        "            model=MODEL_NAME,\n",
        "            model_parameters={\"max_length\": max_length, \"temperature\": 0.7}\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            inputs = self.tokenizer.encode(prompt + self.tokenizer.eos_token, return_tensors=\"pt\")\n",
        "            inputs = inputs.to(self.device)\n",
        "\n",
        "            # Generate response\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs,\n",
        "                    max_length=max_length,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    top_p=0.9\n",
        "                )\n",
        "\n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            response = response.replace(prompt, \"\").strip()\n",
        "\n",
        "            # Update generation with output\n",
        "            generation.end(\n",
        "                output=response,\n",
        "                usage={\"input_tokens\": len(inputs[0]), \"output_tokens\": len(outputs[0])}\n",
        "            )\n",
        "\n",
        "            # Log quality score\n",
        "            trace.score(\n",
        "                name=\"response_quality\",\n",
        "                value=random.uniform(0.7, 0.95),  # Simulated quality score\n",
        "                comment=\"Automated quality assessment\"\n",
        "            )\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            generation.end(\n",
        "                output=f\"Error: {str(e)}\",\n",
        "                status_message=str(e)\n",
        "            )\n",
        "\n",
        "            trace.score(\n",
        "                name=\"error\",\n",
        "                value=0.0,\n",
        "                comment=f\"Error: {str(e)}\"\n",
        "            )\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "# Initialize the model\n",
        "llm = LightweightLLM()\n",
        "\n",
        "# Mock data for demonstration\n",
        "def generate_mock_metrics():\n",
        "    \"\"\"Generate mock observability metrics\"\"\"\n",
        "    dates = [datetime.now() - timedelta(hours=i) for i in range(24)]\n",
        "\n",
        "    return {\n",
        "        'latency': [random.uniform(0.1, 2.0) for _ in range(24)],\n",
        "        'throughput': [random.randint(50, 200) for _ in range(24)],\n",
        "        'error_rate': [random.uniform(0.01, 0.1) for _ in range(24)],\n",
        "        'quality_score': [random.uniform(0.6, 0.95) for _ in range(24)],\n",
        "        'dates': dates,\n",
        "        'token_usage': [random.randint(100, 500) for _ in range(24)]\n",
        "    }\n",
        "\n",
        "def create_latency_chart():\n",
        "    \"\"\"Create latency visualization\"\"\"\n",
        "    metrics = generate_mock_metrics()\n",
        "\n",
        "    fig = px.line(\n",
        "        x=metrics['dates'],\n",
        "        y=metrics['latency'],\n",
        "        title='Response Latency (seconds)',\n",
        "        labels={'x': 'Time', 'y': 'Latency (s)'}\n",
        "    )\n",
        "    fig.update_traces(line=dict(color='blue'))\n",
        "    return fig\n",
        "\n",
        "def create_throughput_chart():\n",
        "    \"\"\"Create throughput visualization\"\"\"\n",
        "    metrics = generate_mock_metrics()\n",
        "\n",
        "    fig = px.bar(\n",
        "        x=metrics['dates'],\n",
        "        y=metrics['throughput'],\n",
        "        title='Requests Throughput',\n",
        "        labels={'x': 'Time', 'y': 'Requests per hour'}\n",
        "    )\n",
        "    fig.update_traces(marker_color='green')\n",
        "    return fig\n",
        "\n",
        "def create_quality_chart():\n",
        "    \"\"\"Create quality score visualization\"\"\"\n",
        "    metrics = generate_mock_metrics()\n",
        "\n",
        "    fig = px.scatter(\n",
        "        x=metrics['dates'],\n",
        "        y=metrics['quality_score'],\n",
        "        title='Response Quality Scores',\n",
        "        labels={'x': 'Time', 'y': 'Quality Score (0-1)'}\n",
        "    )\n",
        "    fig.update_traces(marker=dict(color='orange', size=10))\n",
        "    return fig\n",
        "\n",
        "def create_comprehensive_dashboard():\n",
        "    \"\"\"Create a comprehensive dashboard with multiple metrics\"\"\"\n",
        "    metrics = generate_mock_metrics()\n",
        "\n",
        "    fig = make_subplots(\n",
        "        rows=3, cols=2,\n",
        "        subplot_titles=('Latency', 'Throughput', 'Error Rate', 'Quality Score', 'Token Usage', 'Summary'),\n",
        "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        "    )\n",
        "\n",
        "    # Latency\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=metrics['dates'], y=metrics['latency'], name=\"Latency\", line=dict(color='blue')),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # Throughput\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=metrics['dates'], y=metrics['throughput'], name=\"Throughput\", marker_color='green'),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    # Error Rate\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=metrics['dates'], y=metrics['error_rate'], name=\"Error Rate\", line=dict(color='red')),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    # Quality Score\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=metrics['dates'], y=metrics['quality_score'], name=\"Quality\", line=dict(color='orange')),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "    # Token Usage\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=metrics['dates'], y=metrics['token_usage'], name=\"Tokens\", marker_color='purple'),\n",
        "        row=3, col=1\n",
        "    )\n",
        "\n",
        "    # Summary stats\n",
        "    summary_stats = [\n",
        "        f\"Avg Latency: {np.mean(metrics['latency']):.2f}s\",\n",
        "        f\"Avg Throughput: {np.mean(metrics['throughput']):.0f}/hr\",\n",
        "        f\"Avg Error Rate: {np.mean(metrics['error_rate']):.2%}\",\n",
        "        f\"Avg Quality: {np.mean(metrics['quality_score']):.2f}\"\n",
        "    ]\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=[0, 1, 2, 3],\n",
        "            y=[0, 0, 0, 0],\n",
        "            text=summary_stats,\n",
        "            mode=\"text\",\n",
        "            textposition=\"middle center\",\n",
        "            showlegend=False\n",
        "        ),\n",
        "        row=3, col=2\n",
        "    )\n",
        "\n",
        "    fig.update_layout(height=800, showlegend=True, title_text=\"LLM Observability Dashboard\")\n",
        "    return fig\n",
        "\n",
        "def chat_with_llm(message, history):\n",
        "    \"\"\"Chat interface with the LLM\"\"\"\n",
        "    response = llm.generate_response(message)\n",
        "    return response\n",
        "\n",
        "def get_recent_traces():\n",
        "    \"\"\"Get recent traces from Langfuse (mock data for demo)\"\"\"\n",
        "    try:\n",
        "        # For demo purposes, we'll create mock trace data\n",
        "        # In a real implementation, you would use: traces = langfuse_client.get_traces(limit=10)\n",
        "        trace_data = []\n",
        "        for i in range(10):\n",
        "            trace_data.append({\n",
        "                'id': f\"trace-{i}\",\n",
        "                'name': f\"Generation {i}\",\n",
        "                'timestamp': (datetime.now() - timedelta(minutes=random.randint(1, 60))).isoformat(),\n",
        "                'user_id': f\"user-{random.randint(1, 5)}\",\n",
        "                'metadata': json.dumps({\"model\": MODEL_NAME, \"status\": \"completed\"})\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(trace_data)\n",
        "\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame({'Error': [f\"Failed to fetch traces: {str(e)}\"]})\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks(title=\"LLM Observability Dashboard\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# üöÄ LLM Observability Dashboard\")\n",
        "    gr.Markdown(\"Monitor your LLM performance with real-time metrics and analytics\")\n",
        "\n",
        "    with gr.Tab(\"Live Metrics\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                latency_plot = gr.Plot(label=\"Latency\", value=create_latency_chart())\n",
        "            with gr.Column():\n",
        "                throughput_plot = gr.Plot(label=\"Throughput\", value=create_throughput_chart())\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                quality_plot = gr.Plot(label=\"Quality Scores\", value=create_quality_chart())\n",
        "            with gr.Column():\n",
        "                comprehensive_dash = gr.Plot(label=\"Comprehensive Dashboard\", value=create_comprehensive_dashboard())\n",
        "\n",
        "        refresh_btn = gr.Button(\"üîÑ Refresh Metrics\")\n",
        "\n",
        "    with gr.Tab(\"Chat Interface\"):\n",
        "        gr.Markdown(\"### Test the LLM with real-time monitoring\")\n",
        "        chatbot = gr.ChatInterface(\n",
        "            fn=chat_with_llm,\n",
        "            title=\"LLM Chat with Observability\",\n",
        "            description=\"Chat with the model while monitoring performance metrics\"\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"Langfuse Traces\"):\n",
        "        gr.Markdown(\"### Recent Traces from Langfuse\")\n",
        "        traces_table = gr.Dataframe(\n",
        "            label=\"Recent Traces\",\n",
        "            value=get_recent_traces,\n",
        "            every=30  # Refresh every 30 seconds\n",
        "        )\n",
        "        refresh_traces = gr.Button(\"üîÑ Refresh Traces\")\n",
        "\n",
        "    with gr.Tab(\"Model Info\"):\n",
        "        gr.Markdown(f\"### Model: {MODEL_NAME}\")\n",
        "        gr.Markdown(f\"**Device:** {llm.device}\")\n",
        "        gr.Markdown(f\"**Parameters:** ~{llm.model.num_parameters():,}\")\n",
        "        gr.Markdown(\"**Capabilities:** Text generation, conversation\")\n",
        "\n",
        "        # Model statistics\n",
        "        stats_data = {\n",
        "            'Metric': ['Avg Latency', 'Peak Throughput', 'Error Rate', 'Avg Quality'],\n",
        "            'Value': ['0.8s', '200 req/hr', '2.5%', '0.85'],\n",
        "            'Status': ['‚úÖ Good', '‚úÖ Good', '‚ö†Ô∏è Warning', '‚úÖ Good']\n",
        "        }\n",
        "        stats_df = pd.DataFrame(stats_data)\n",
        "        gr.Dataframe(stats_df, label=\"Current Performance\")\n",
        "\n",
        "    # Refresh functionality\n",
        "    def refresh_all():\n",
        "        return [\n",
        "            create_latency_chart(),\n",
        "            create_throughput_chart(),\n",
        "            create_quality_chart(),\n",
        "            create_comprehensive_dashboard(),\n",
        "            get_recent_traces()\n",
        "        ]\n",
        "\n",
        "    refresh_btn.click(\n",
        "        fn=refresh_all,\n",
        "        outputs=[latency_plot, throughput_plot, quality_plot, comprehensive_dash, traces_table]\n",
        "    )\n",
        "\n",
        "    refresh_traces.click(\n",
        "        fn=get_recent_traces,\n",
        "        outputs=traces_table\n",
        "    )\n",
        "\n",
        "# Launch the dashboard\n",
        "if __name__ == \"__main__\":\n",
        "    # This will open in a new tab when run locally\n",
        "    demo.launch(\n",
        "        share=True,  # Set to True if you want a public link\n",
        "        server_name=\"localhost\",\n",
        "        server_port=8083,\n",
        "        show_error=True,\n",
        "        #debug=True\n",
        "\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "UQX1Jyu7Z1M7",
        "outputId": "d6355fe2-b9e7-49e2-9776-42ca7e0255fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b85c651f2febe2e95b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b85c651f2febe2e95b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-AJq-c1LZ-VK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}