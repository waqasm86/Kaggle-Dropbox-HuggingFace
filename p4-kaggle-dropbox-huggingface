{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install huggingface_hub > /dev/null","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:46:34.568480Z","iopub.execute_input":"2025-08-20T06:46:34.569044Z","iopub.status.idle":"2025-08-20T06:46:39.691326Z","shell.execute_reply.started":"2025-08-20T06:46:34.569018Z","shell.execute_reply":"2025-08-20T06:46:39.690349Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install openal > /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:58:40.726906Z","iopub.execute_input":"2025-08-20T06:58:40.727544Z","iopub.status.idle":"2025-08-20T06:58:42.578698Z","shell.execute_reply.started":"2025-08-20T06:58:40.727518Z","shell.execute_reply":"2025-08-20T06:58:42.577916Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement openal (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for openal\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from huggingface_hub import login, logout, whoami","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:46:40.609131Z","iopub.execute_input":"2025-08-20T06:46:40.609408Z","iopub.status.idle":"2025-08-20T06:46:41.106740Z","shell.execute_reply.started":"2025-08-20T06:46:40.609381Z","shell.execute_reply":"2025-08-20T06:46:41.106016Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nHF_TOKEN = UserSecretsClient().get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:46:41.263188Z","iopub.execute_input":"2025-08-20T06:46:41.263415Z","iopub.status.idle":"2025-08-20T06:46:41.375554Z","shell.execute_reply.started":"2025-08-20T06:46:41.263396Z","shell.execute_reply":"2025-08-20T06:46:41.374885Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"login(token=HF_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:46:41.734964Z","iopub.execute_input":"2025-08-20T06:46:41.735229Z","iopub.status.idle":"2025-08-20T06:46:41.884588Z","shell.execute_reply.started":"2025-08-20T06:46:41.735209Z","shell.execute_reply":"2025-08-20T06:46:41.884125Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# For GPU support\n!pip install -q -U transformers==4.45 accelerate bitsandbytes  > /dev/null\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:46:42.653743Z","iopub.execute_input":"2025-08-20T06:46:42.653957Z","iopub.status.idle":"2025-08-20T06:48:17.036704Z","shell.execute_reply.started":"2025-08-20T06:46:42.653941Z","shell.execute_reply":"2025-08-20T06:48:17.035823Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# Check if we have multiple GPUs\nprint(f\"Number of GPUs available: {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n\n# Configure 4-bit quantization to drastically reduce VRAM usage\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16 # Use FP16 for faster computation\n)\n\n# Load the model and let 'accelerate' automatically distribute it across available devices\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-3.1-8B-Instruct\",\n    quantization_config=quantization_config,\n    device_map=\"auto\",  # The key to multi-GPU and CPU offloading\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n# Add a padding token if it doesn't exist (for batch inference later)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Define the chat message\nmessages = [\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\n# Apply the chat template and prepare inputs\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_tensors=\"pt\", # Return PyTorch tensors\n)\n\n# Move inputs to the same device as the model's main layer\ninputs = inputs.to(model.device)\n\n# Generate a response\nwith torch.no_grad(): # Reduces memory usage during generation\n    outputs = model.generate(\n        input_ids=inputs,\n        max_new_tokens=256, # Increased for a more complete answer\n        do_sample=True, # Enable sampling for less repetitive answers\n        temperature=0.7, # Control randomness: lower is more deterministic\n        top_p=0.9, # Nucleus sampling: choose from top p probability mass\n        pad_token_id=tokenizer.eos_token_id # Set pad token for generation\n    )\n\n# Decode only the new tokens (skip the prompt)\nresponse = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\nprint(\"Model's response:\")\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:50:16.180704Z","iopub.execute_input":"2025-08-20T06:50:16.181494Z","iopub.status.idle":"2025-08-20T06:54:04.350322Z","shell.execute_reply.started":"2025-08-20T06:50:16.181463Z","shell.execute_reply":"2025-08-20T06:54:04.349532Z"}},"outputs":[{"name":"stdout","text":"Number of GPUs available: 2\nGPU 0: Tesla T4\nGPU 1: Tesla T4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b6e51c9b26044aabec34fb637b01d60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb6d5df97a1d41a0ac44e97c09f611f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"321ba678a29645db89eb7cf397a13cc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddc20d0715b841dea6d64d9f240e3bef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bebd943ddda43aca02c8f6c452be829"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e177761c7c848d38136cccb927ffd27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97bf83d73d6d40f596b8d7b7787719bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5f85cbae7ed4ce6bcc885ed6cceb2d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66ac4721188e4cb998f780aeba8c7ff2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d948110f0a943e9aa53881cf7c493e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bf1b0f51e23406bb2cdf601e401fd6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaf9bb91164343848e468a50195b5e83"}},"metadata":{}},{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n2025-08-20 06:53:38.163586: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755672818.497295      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755672818.593202      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Model's response:\nI’m a large language model, so I don’t have a personal identity in the classical sense. However, I can provide some information about my capabilities and what I can do for you. I am a type of artificial intelligence (AI) that is designed to process and generate human-like text based on the input I receive. \n\nI can answer questions, provide information, and engage in conversations on a wide range of topics. I can also generate text, summarize content, and even create simple stories or poems. I don’t have personal opinions or feelings, but I can provide information and insights that might be helpful or interesting.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"https://router.huggingface.co/v1\",\n    api_key=HF_TOKEN,\n)\n\ncompletion = client.chat.completions.create(\n    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the capital of France?\"\n        }\n    ],\n)\n\nprint(completion.choices[0].message)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T07:02:47.739898Z","iopub.execute_input":"2025-08-20T07:02:47.740554Z","iopub.status.idle":"2025-08-20T07:02:52.098453Z","shell.execute_reply.started":"2025-08-20T07:02:47.740529Z","shell.execute_reply":"2025-08-20T07:02:52.097684Z"}},"outputs":[{"name":"stdout","text":"ChatCompletionMessage(content='The capital of France is Paris.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}