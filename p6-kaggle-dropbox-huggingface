{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install huggingface_hub > /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T05:59:08.221529Z","iopub.execute_input":"2025-08-21T05:59:08.221761Z","iopub.status.idle":"2025-08-21T05:59:12.115956Z","shell.execute_reply.started":"2025-08-21T05:59:08.221744Z","shell.execute_reply":"2025-08-21T05:59:12.115210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install openai > /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T05:59:12.117371Z","iopub.execute_input":"2025-08-21T05:59:12.117604Z","iopub.status.idle":"2025-08-21T05:59:15.156109Z","shell.execute_reply.started":"2025-08-21T05:59:12.117582Z","shell.execute_reply":"2025-08-21T05:59:15.155353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login, logout, whoami","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T05:59:15.157198Z","iopub.execute_input":"2025-08-21T05:59:15.157484Z","iopub.status.idle":"2025-08-21T05:59:15.582445Z","shell.execute_reply.started":"2025-08-21T05:59:15.157434Z","shell.execute_reply":"2025-08-21T05:59:15.581893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nHF_TOKEN = UserSecretsClient().get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T05:59:15.583668Z","iopub.execute_input":"2025-08-21T05:59:15.584117Z","iopub.status.idle":"2025-08-21T05:59:15.724811Z","shell.execute_reply.started":"2025-08-21T05:59:15.584097Z","shell.execute_reply":"2025-08-21T05:59:15.724330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"login(token=HF_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T05:59:15.725401Z","iopub.execute_input":"2025-08-21T05:59:15.725570Z","iopub.status.idle":"2025-08-21T05:59:15.866546Z","shell.execute_reply.started":"2025-08-21T05:59:15.725557Z","shell.execute_reply":"2025-08-21T05:59:15.865814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For GPU support\n!pip install -q -U transformers==4.45 accelerate bitsandbytes  > /dev/null\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T06:01:11.391296Z","iopub.execute_input":"2025-08-21T06:01:11.392036Z","iopub.status.idle":"2025-08-21T06:01:14.808967Z","shell.execute_reply.started":"2025-08-21T06:01:11.392003Z","shell.execute_reply":"2025-08-21T06:01:14.808195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# Check if we have multiple GPUs\nprint(f\"Number of GPUs available: {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n\n# Configure 4-bit quantization to drastically reduce VRAM usage\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16 # Use FP16 for faster computation\n)\n\n# Load the model and let 'accelerate' automatically distribute it across available devices\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-3.1-8B-Instruct\",\n    quantization_config=quantization_config,\n    device_map=\"auto\",  # The key to multi-GPU and CPU offloading\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n# Add a padding token if it doesn't exist (for batch inference later)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Define the chat message\nmessages = [\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\n# Apply the chat template and prepare inputs\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_tensors=\"pt\", # Return PyTorch tensors\n)\n\n# Move inputs to the same device as the model's main layer\ninputs = inputs.to(model.device)\n\n# Generate a response\nwith torch.no_grad(): # Reduces memory usage during generation\n    outputs = model.generate(\n        input_ids=inputs,\n        max_new_tokens=256, # Increased for a more complete answer\n        do_sample=True, # Enable sampling for less repetitive answers\n        temperature=0.7, # Control randomness: lower is more deterministic\n        top_p=0.9, # Nucleus sampling: choose from top p probability mass\n        pad_token_id=tokenizer.eos_token_id # Set pad token for generation\n    )\n\n# Decode only the new tokens (skip the prompt)\nresponse = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\nprint(\"Model's response:\")\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:50:16.180704Z","iopub.execute_input":"2025-08-20T06:50:16.181494Z","iopub.status.idle":"2025-08-20T06:54:04.350322Z","shell.execute_reply.started":"2025-08-20T06:50:16.181463Z","shell.execute_reply":"2025-08-20T06:54:04.349532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###Approach 1###\n\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport gc\n\n# Check available devices\nprint(f\"Available GPUs: {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)} - Memory: {torch.cuda.get_device_properties(i).total_memory/1024**3:.1f}GB\")\n\n# Configuration\nmodel_name = \"defog/sqlcoder-7b-2\"\nmax_memory = {\n    0: \"12GB\",\n    1: \"12GB\",\n    \"cpu\": \"20GB\"\n}\n\n# Load tokenizer and set padding token\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token\n\n# Load model directly\nprint(\"Loading model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    max_memory=max_memory,\n    low_cpu_mem_usage=True\n)\n\nprint(f\"Model device map: {model.hf_device_map}\")\n\ndef generate_sql_query(question):\n    \"\"\"Generate SQL query using SQLCoder's preferred format\"\"\"\n    prompt = f\"\"\"### Task\nGenerate a SQL query to answer the following question:\n{question}\n\n### SQL\n\"\"\"\n    \n    # Tokenize input without padding\n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=512,\n        padding=False  # No padding needed for single sequences\n    )\n    \n    # Move inputs to the same device as the model\n    input_device = next(model.parameters()).device\n    inputs = {k: v.to(input_device) for k, v in inputs.items()}\n    \n    # Generate text\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            do_sample=True,\n            temperature=0.3,\n            pad_token_id=tokenizer.pad_token_id,  # Use the set pad token\n            repetition_penalty=1.1,\n            num_return_sequences=1\n        )\n    \n    # Extract only the generated SQL part (after the prompt)\n    generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n    return generated_text.strip()\n\n# Test queries\nqueries = [\n    \"Write a SQL query to find all customers who made purchases in the last 30 days.\",\n    \"Write a SQL query to find the top 5 products by sales volume.\",\n    \"Write a SQL query to calculate total revenue by month for the current year.\",\n    \"Write a SQL query to find customers who haven't made a purchase in the last 6 months.\"\n]\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"SQL QUERY GENERATION RESULTS\")\nprint(\"=\"*80)\n\nfor i, question in enumerate(queries, 1):\n    print(f\"\\n{i}. Question: {question}\")\n    try:\n        result = generate_sql_query(question)\n        print(f\"   Generated SQL:\\n   {result}\")\n    except Exception as e:\n        print(f\"   Error: {e}\")\n        import traceback\n        traceback.print_exc()\n\n# Memory usage info\nprint(f\"\\n\" + \"=\"*80)\nprint(\"MEMORY USAGE INFO\")\nprint(\"=\"*80)\nfor i in range(torch.cuda.device_count()):\n    allocated = torch.cuda.memory_allocated(i)/1024**3\n    cached = torch.cuda.memory_reserved(i)/1024**3\n    total = torch.cuda.get_device_properties(i).total_memory/1024**3\n    print(f\"GPU {i}: {allocated:.2f}GB allocated, {cached:.2f}GB cached / {total:.2f}GB total ({allocated/total*100:.1f}%)\")\n\n# Clean up\ndel model\ntorch.cuda.empty_cache()\ngc.collect()\n\nprint(f\"\\nAfter cleanup:\")\nfor i in range(torch.cuda.device_count()):\n    allocated = torch.cuda.memory_allocated(i)/1024**3\n    print(f\"GPU {i}: {allocated:.2f}GB allocated\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T05:55:28.290430Z","iopub.execute_input":"2025-08-21T05:55:28.291024Z","iopub.status.idle":"2025-08-21T05:55:51.079914Z","shell.execute_reply.started":"2025-08-21T05:55:28.291001Z","shell.execute_reply":"2025-08-21T05:55:51.079197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###Approach 1 Optimized###\n\n\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport gc\n\nclass SQLCoderGenerator:\n    def __init__(self, model_name=\"defog/sqlcoder-7b-2\"):\n        self.model_name = model_name\n        self.device_map = \"auto\"\n        self.max_memory = {0: \"12GB\", 1: \"12GB\", \"cpu\": \"20GB\"}\n        \n        # Load tokenizer and set padding token\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        # Load model\n        print(\"Loading SQLCoder model...\")\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            device_map=self.device_map,\n            torch_dtype=torch.float16,\n            max_memory=self.max_memory,\n            low_cpu_mem_usage=True\n        )\n        print(f\"Model loaded on devices: {self.model.hf_device_map}\")\n    \n    def generate_sql(self, question, max_new_tokens=150):\n        \"\"\"Generate SQL query for a given question\"\"\"\n        prompt = f\"\"\"### Task\nGenerate a SQL query to answer the following question:\n{question}\n\n### SQL\n\"\"\"\n        \n        # Tokenize input\n        inputs = self.tokenizer(\n            prompt,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=512,\n            padding=False\n        )\n        \n        # Move inputs to model device\n        input_device = next(self.model.parameters()).device\n        inputs = {k: v.to(input_device) for k, v in inputs.items()}\n        \n        # Generate SQL\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                do_sample=True,\n                temperature=0.3,\n                pad_token_id=self.tokenizer.pad_token_id,\n                repetition_penalty=1.1\n            )\n        \n        # Extract and clean the generated SQL\n        generated_text = self.tokenizer.decode(\n            outputs[0][inputs[\"input_ids\"].shape[1]:], \n            skip_special_tokens=True\n        )\n        return generated_text.strip()\n    \n    def cleanup(self):\n        \"\"\"Clean up memory\"\"\"\n        del self.model\n        torch.cuda.empty_cache()\n        gc.collect()\n\n# Usage example\nif __name__ == \"__main__\":\n    # Initialize generator\n    sql_generator = SQLCoderGenerator()\n    \n    # Test queries\n    test_queries = [\n        \"Write a SQL query to find all customers who made purchases in the last 30 days.\",\n        \"Write a SQL query to find the top 5 products by sales volume.\",\n        \"Write a SQL query to calculate total revenue by month for the current year.\",\n        \"Write a SQL query to find customers who haven't made a purchase in the last 6 months.\",\n        \"Write a SQL query to get the average order value by customer segment.\"\n    ]\n    \n    print(\"=\" * 80)\n    print(\"SQL CODER GENERATION RESULTS\")\n    print(\"=\" * 80)\n    \n    for i, question in enumerate(test_queries, 1):\n        print(f\"\\n{i}. {question}\")\n        sql_query = sql_generator.generate_sql(question)\n        print(f\"   \\033[92m{sql_query}\\033[0m\")  # Green color for SQL\n    \n    # Memory info\n    print(f\"\\n{'='*80}\")\n    print(\"MEMORY USAGE\")\n    print(f\"{'='*80}\")\n    for i in range(torch.cuda.device_count()):\n        allocated = torch.cuda.memory_allocated(i)/1024**3\n        total = torch.cuda.get_device_properties(i).total_memory/1024**3\n        print(f\"GPU {i}: {allocated:.2f}GB / {total:.2f}GB ({allocated/total*100:.1f}%)\")\n    \n    # Cleanup\n    sql_generator.cleanup()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T06:07:28.780516Z","iopub.execute_input":"2025-08-21T06:07:28.781240Z","iopub.status.idle":"2025-08-21T06:07:54.881157Z","shell.execute_reply.started":"2025-08-21T06:07:28.781214Z","shell.execute_reply":"2025-08-21T06:07:54.880610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"https://router.huggingface.co/v1\",\n    api_key=HF_TOKEN,\n)\n\ncompletion = client.chat.completions.create(\n    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the capital of France?\"\n        }\n    ],\n)\n\nprint(completion.choices[0].message)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T07:02:47.739898Z","iopub.execute_input":"2025-08-20T07:02:47.740554Z","iopub.status.idle":"2025-08-20T07:02:52.098453Z","shell.execute_reply.started":"2025-08-20T07:02:47.740529Z","shell.execute_reply":"2025-08-20T07:02:52.097684Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}