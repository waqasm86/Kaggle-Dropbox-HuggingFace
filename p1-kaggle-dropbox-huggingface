{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install -U \"transformers>=4.44\" accelerate langfuse huggingface_hub dropbox requests tqdm> /dev/null\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T11:04:44.365215Z","iopub.execute_input":"2025-08-18T11:04:44.365538Z","iopub.status.idle":"2025-08-18T11:06:18.824952Z","shell.execute_reply.started":"2025-08-18T11:04:44.365498Z","shell.execute_reply":"2025-08-18T11:06:18.824239Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.32.0 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.32.0 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.32.0 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token=\"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\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T11:06:26.784166Z","iopub.execute_input":"2025-08-18T11:06:26.784484Z","iopub.status.idle":"2025-08-18T11:06:27.236269Z","shell.execute_reply.started":"2025-08-18T11:06:26.784459Z","shell.execute_reply":"2025-08-18T11:06:27.235704Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\n\nos.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-186192e8-9f2b-41ad-be1e-109efad2d3a3\"\nos.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-194ee11a-d986-41e0-b6d7-6b4e0415291e\"\nos.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"  # or your region/self-host\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T11:06:27.383420Z","iopub.execute_input":"2025-08-18T11:06:27.383627Z","iopub.status.idle":"2025-08-18T11:06:27.387410Z","shell.execute_reply.started":"2025-08-18T11:06:27.383610Z","shell.execute_reply":"2025-08-18T11:06:27.386720Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from langfuse import Langfuse\n\nlangfuse = Langfuse(\n  secret_key=\"sk-lf-194ee11a-d986-41e0-b6d7-6b4e0415291e\",\n  public_key=\"pk-lf-186192e8-9f2b-41ad-be1e-109efad2d3a3\",\n  host=\"https://us.cloud.langfuse.com\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T11:06:28.126511Z","iopub.execute_input":"2025-08-18T11:06:28.126874Z","iopub.status.idle":"2025-08-18T11:06:29.518026Z","shell.execute_reply.started":"2025-08-18T11:06:28.126857Z","shell.execute_reply":"2025-08-18T11:06:29.517519Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"HF_TOKEN = \"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\"\nDROPBOX_ACCESS_TOKEN=\"sl.u.AF5bmQnc57vpaG6t89Zc6ujRFL3QipS4Hhl6E4_7_JuKsNwyAyeehFYm_iE-i9j9K4Yn2d5j_96DKy2eDKnyMGjFu3EKIFLW52WrEzQZxA46xeMBU6Pd4L7U0wtd1t0ZcWeLXSFIzY18dxgI59uFZHUTUhy9YkiACoAOp4rt9_8Us_0IbD6XZXK4JK6NLpnaXDK576Y0PpvHR26toYYTlpHcZr_mmIQ6csSieUzZDF1shGM402uCdacPifbfzjklJmB9JOAZYM3zCSumNoPWyd6rx4YzHaOkg2yOVHCwwhD5dwQb46kJ-vYTtUdw4jhT7wi4jglIqp3p6L8rbNMQi4dgAs55j_heP-yiouyJIUoMjNnmf43kjBkaYUwOkUDqeb-4l2WWvUaTo_1l-_TNBOrjy43fyIGJRczs5_yH_-A3Oktl04YciK1EWnMkETcNFNReIlOyiH9yZgEGEn9gwScnb5mBdbpUNP9E6Y4QapqY9FYj8WN1hv15rKJ1NVUYgtYhzElx7kLhLiWte5kiGptuwlSmUEpJSt_DPto29IbbIgFsA8YRelMSmlRUnUQvFYGUhUzFHAbVuk_KhSk-eNTFt7dMzDgI98VEjh2AK_nQGQondGgd4LAOa4koQeOEjD3SrMbV_Q9hh4seL8FXbfBg74CGWQvThk9Xl0KSoAjciQ5bL3xfYb1mh2wQf2WiPSEOwitqAQRZGqFaQBWuv8GRTsqp_77s5Tl4uWBBr0Pi0gQjUvatXrOylgydFafZa4set7Fa-CTjYBKhBYZ0IYpR4e-FewDpcCqrO8_D66txBuwENa4sg77g29058S-CiJdeQlm4cOOEv4DhjT6FXRMEqiGalmtj0XIkeDRVQDKJzjTirsnq9AUpT24izL7-jyXrBLr3gc_LFCdAxzGAVTOLEEX7CWaeDIAco8IhI6wWnbjkhhJqdGzPboltCOCgpjCIc7W_h7LKeA3QMR5-ZQVXedmh70IHjaAJT7QO7hSfoom8dOFV5I5MGbGYxutEggwtQ2z3SobhrtsfTb1fVTxzdUk7i4aLHQkxQhvtin1WPr01f2d_hW3KJ59T7U_5eEzAo1M3mfW6alANPieO3zSZbRc9WVw8q3BLICkE34eFi-pwBzBNMShFs2CNYv4eP4wblXF2Y1ln3kVyGxLImAxNN6SLcL2aQJZsmgdt7jeKxkRZvrH89tzowrti0bVpx6fjXOpdZbTAInBznLrmWZARsvBAZywTB2iJP0lHzaO_Xi8s2kUb8jbDIl9fXObmqGQ\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T11:06:55.851138Z","iopub.execute_input":"2025-08-18T11:06:55.851558Z","iopub.status.idle":"2025-08-18T11:06:55.855328Z","shell.execute_reply.started":"2025-08-18T11:06:55.851539Z","shell.execute_reply":"2025-08-18T11:06:55.854690Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import dropbox\nfrom dropbox.exceptions import AuthError\n\ndef dropbox_connect():\n    try:\n        dbx = dropbox.Dropbox(DROPBOX_ACCESS_TOKEN)\n        dbx.users_get_current_account()  # Test connection\n        print(\"Successfully connected to Dropbox!\")\n        return dbx\n    except AuthError as e:\n        print(\"Error connecting to Dropbox:\", e)\n        return None\n\ndbx = dropbox_connect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T11:06:56.329575Z","iopub.execute_input":"2025-08-18T11:06:56.330165Z","iopub.status.idle":"2025-08-18T11:06:56.799358Z","shell.execute_reply.started":"2025-08-18T11:06:56.330145Z","shell.execute_reply":"2025-08-18T11:06:56.798768Z"}},"outputs":[{"name":"stdout","text":"Successfully connected to Dropbox!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def upload_to_dropbox(local_path, dropbox_path):\n    dbx = dropbox.Dropbox(\n        DROPBOX_ACCESS_TOKEN,\n        timeout=30  # Prevents timeout errors\n    )\n    try:\n        for root, dirs, files in os.walk(local_path):\n            for file in files:\n                local_file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(local_file_path, local_path)\n                dropbox_file_path = os.path.join(dropbox_path, relative_path).replace(\"\\\\\", \"/\")  # Fix path format\n                \n                # Chunked upload for large files (avoids memory issues)\n                CHUNK_SIZE = 4 * 1024 * 1024  # 4MB chunks\n                with open(local_file_path, \"rb\") as f:\n                    file_size = os.path.getsize(local_file_path)\n                    if file_size <= CHUNK_SIZE:\n                        dbx.files_upload(f.read(), dropbox_file_path, mode=dropbox.files.WriteMode(\"overwrite\"))\n                    else:\n                        upload_session_start_result = dbx.files_upload_session_start(f.read(CHUNK_SIZE))\n                        cursor = dropbox.files.UploadSessionCursor(\n                            session_id=upload_session_start_result.session_id,\n                            offset=f.tell()\n                        )\n                        while f.tell() < file_size:\n                            remaining = file_size - f.tell()\n                            chunk = f.read(min(CHUNK_SIZE, remaining))\n                            dbx.files_upload_session_append_v2(chunk, cursor)\n                            cursor.offset = f.tell()\n                        dbx.files_upload_session_finish(None, cursor, dropbox.files.CommitInfo(path=dropbox_file_path))\n                print(f\"Uploaded: {dropbox_file_path}\")\n    except Exception as e:\n        print(f\"Error uploading to Dropbox: {e}\")\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:28:02.212763Z","iopub.execute_input":"2025-08-18T10:28:02.213389Z","iopub.status.idle":"2025-08-18T10:28:02.220268Z","shell.execute_reply.started":"2025-08-18T10:28:02.213365Z","shell.execute_reply":"2025-08-18T10:28:02.219596Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\n\n# Imports\nfrom huggingface_hub import snapshot_download\nimport dropbox\nimport os\n\n# Hugging Face model (adjust if needed)\nmodel_id = \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\nlocal_model_dir = \"./downloaded_model\"\n\n# Dropbox setup\nDROPBOX_ACCESS_TOKEN=\"sl.u.AF5bmQnc57vpaG6t89Zc6ujRFL3QipS4Hhl6E4_7_JuKsNwyAyeehFYm_iE-i9j9K4Yn2d5j_96DKy2eDKnyMGjFu3EKIFLW52WrEzQZxA46xeMBU6Pd4L7U0wtd1t0ZcWeLXSFIzY18dxgI59uFZHUTUhy9YkiACoAOp4rt9_8Us_0IbD6XZXK4JK6NLpnaXDK576Y0PpvHR26toYYTlpHcZr_mmIQ6csSieUzZDF1shGM402uCdacPifbfzjklJmB9JOAZYM3zCSumNoPWyd6rx4YzHaOkg2yOVHCwwhD5dwQb46kJ-vYTtUdw4jhT7wi4jglIqp3p6L8rbNMQi4dgAs55j_heP-yiouyJIUoMjNnmf43kjBkaYUwOkUDqeb-4l2WWvUaTo_1l-_TNBOrjy43fyIGJRczs5_yH_-A3Oktl04YciK1EWnMkETcNFNReIlOyiH9yZgEGEn9gwScnb5mBdbpUNP9E6Y4QapqY9FYj8WN1hv15rKJ1NVUYgtYhzElx7kLhLiWte5kiGptuwlSmUEpJSt_DPto29IbbIgFsA8YRelMSmlRUnUQvFYGUhUzFHAbVuk_KhSk-eNTFt7dMzDgI98VEjh2AK_nQGQondGgd4LAOa4koQeOEjD3SrMbV_Q9hh4seL8FXbfBg74CGWQvThk9Xl0KSoAjciQ5bL3xfYb1mh2wQf2WiPSEOwitqAQRZGqFaQBWuv8GRTsqp_77s5Tl4uWBBr0Pi0gQjUvatXrOylgydFafZa4set7Fa-CTjYBKhBYZ0IYpR4e-FewDpcCqrO8_D66txBuwENa4sg77g29058S-CiJdeQlm4cOOEv4DhjT6FXRMEqiGalmtj0XIkeDRVQDKJzjTirsnq9AUpT24izL7-jyXrBLr3gc_LFCdAxzGAVTOLEEX7CWaeDIAco8IhI6wWnbjkhhJqdGzPboltCOCgpjCIc7W_h7LKeA3QMR5-ZQVXedmh70IHjaAJT7QO7hSfoom8dOFV5I5MGbGYxutEggwtQ2z3SobhrtsfTb1fVTxzdUk7i4aLHQkxQhvtin1WPr01f2d_hW3KJ59T7U_5eEzAo1M3mfW6alANPieO3zSZbRc9WVw8q3BLICkE34eFi-pwBzBNMShFs2CNYv4eP4wblXF2Y1ln3kVyGxLImAxNN6SLcL2aQJZsmgdt7jeKxkRZvrH89tzowrti0bVpx6fjXOpdZbTAInBznLrmWZARsvBAZywTB2iJP0lHzaO_Xi8s2kUb8jbDIl9fXObmqGQ\"\nDROPBOX_UPLOAD_PATH = \"/gpt-oss-20b\"\n\n# Download model\nprint(\"Downloading model...\")\nsnapshot_download(\n    repo_id=model_id,\n    local_dir=local_model_dir,\n    token=True,  # Required for gated models\n    ignore_patterns=[\"*.bin\"]  # Skip large binaries if needed\n)\n\n# Upload to Dropbox (using fixed function)\nprint(\"Uploading to Dropbox...\")\nupload_to_dropbox(local_model_dir, DROPBOX_UPLOAD_PATH)\n\n# Cleanup\n!rm -rf $local_model_dir\nprint(\"Done! Model saved to Dropbox.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:33:13.231893Z","iopub.execute_input":"2025-08-18T10:33:13.232594Z","iopub.status.idle":"2025-08-18T10:33:35.432096Z","shell.execute_reply.started":"2025-08-18T10:33:13.232568Z","shell.execute_reply":"2025-08-18T10:33:35.430866Z"}},"outputs":[{"name":"stdout","text":"Downloading model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c431ab6f217b40a69f4bc605f1076c4f"}},"metadata":{}},{"name":"stdout","text":"Uploading to Dropbox...\nUploaded: /gpt-oss-20b/LICENSE\nUploaded: /gpt-oss-20b/README.md\nUploaded: /gpt-oss-20b/config.json\nUploaded: /gpt-oss-20b/.gitattributes\nUploaded: /gpt-oss-20b/tokenizer_config.json\nUploaded: /gpt-oss-20b/generation_config.json\nUploaded: /gpt-oss-20b/special_tokens_map.json\nUploaded: /gpt-oss-20b/pytorch_model.bin.index.json\nUploaded: /gpt-oss-20b/tokenizer.json\nUploaded: /gpt-oss-20b/.cache/huggingface/.gitignore\nUploaded: /gpt-oss-20b/.cache/huggingface/download/config.json.metadata\nUploaded: /gpt-oss-20b/.cache/huggingface/download/.gitattributes.lock\nUploaded: /gpt-oss-20b/.cache/huggingface/download/config.json.lock\nUploaded: /gpt-oss-20b/.cache/huggingface/download/generation_config.json.lock\nUploaded: /gpt-oss-20b/.cache/huggingface/download/pytorch_model.bin.index.json.metadata\nUploaded: /gpt-oss-20b/.cache/huggingface/download/pytorch_model.bin.index.json.lock\nUploaded: /gpt-oss-20b/.cache/huggingface/download/LICENSE.metadata\nUploaded: /gpt-oss-20b/.cache/huggingface/download/README.md.lock\nUploaded: /gpt-oss-20b/.cache/huggingface/download/.gitattributes.metadata\nUploaded: /gpt-oss-20b/.cache/huggingface/download/LICENSE.lock\nUploaded: /gpt-oss-20b/.cache/huggingface/download/tokenizer.json.lock\nUploaded: /gpt-oss-20b/.cache/huggingface/download/special_tokens_map.json.metadata\nUploaded: /gpt-oss-20b/.cache/huggingface/download/tokenizer_config.json.lock\nUploaded: /gpt-oss-20b/.cache/huggingface/download/tokenizer_config.json.metadata\nUploaded: /gpt-oss-20b/.cache/huggingface/download/tokenizer.json.metadata\nUploaded: /gpt-oss-20b/.cache/huggingface/download/special_tokens_map.json.lock\nUploaded: /gpt-oss-20b/.cache/huggingface/download/generation_config.json.metadata\nUploaded: /gpt-oss-20b/.cache/huggingface/download/README.md.metadata\nDone! Model saved to Dropbox.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"dbx = dropbox.Dropbox(DROPBOX_ACCESS_TOKEN)\nprint(dbx.users_get_current_account())  # Should print your account info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:35:24.615493Z","iopub.execute_input":"2025-08-18T10:35:24.616372Z","iopub.status.idle":"2025-08-18T10:35:24.870263Z","shell.execute_reply.started":"2025-08-18T10:35:24.616341Z","shell.execute_reply":"2025-08-18T10:35:24.869674Z"}},"outputs":[{"name":"stdout","text":"FullAccount(account_id='dbid:AABp6OXGkbpaIZBfrvHfyPkuH9q2F6EJ4lU', account_type=AccountType('pro', None), country='CA', disabled=False, email='waqasm86@gmail.com', email_verified=True, is_paired=False, locale='en', name=Name(abbreviated_name='MW', display_name='M Waqas', familiar_name='M', given_name='M', surname='Waqas'), profile_photo_url=NOT_SET, referral_link='https://www.dropbox.com/referrals/AAAgmZFJ3sdf-qcvwbb-EU2SgKt6k84JXmA?src=app9-5059235', root_info=UserRootInfo(home_namespace_id='208040503', root_namespace_id='208040503'), team=NOT_SET, team_member_id=NOT_SET)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"DROPBOX_FOLDER_PATH = \"/gpt-oss-20b\"\n\ndef delete_dropbox_folder(path):\n    try:\n        # List all files/folders in the target directory\n        result = dbx.files_list_folder(path)\n        \n        # Delete each file/subfolder\n        for entry in result.entries:\n            if isinstance(entry, dropbox.files.FileMetadata):\n                dbx.files_delete(entry.path_display)\n                print(f\"Deleted file: {entry.path_display}\")\n            elif isinstance(entry, dropbox.files.FolderMetadata):\n                # Recursively delete subfolders\n                delete_dropbox_folder(entry.path_display)\n        \n        # Delete the main folder itself\n        dbx.files_delete(path)\n        print(f\"Successfully deleted folder: {path}\")\n    \n    except dropbox.exceptions.ApiError as e:\n        print(f\"Error deleting {path}: {e}\")\n\n# Execute deletion\nprint(f\"Deleting all contents in '{DROPBOX_FOLDER_PATH}'...\")\ndelete_dropbox_folder(DROPBOX_FOLDER_PATH)\nprint(\"Cleanup complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:40:47.551422Z","iopub.execute_input":"2025-08-18T10:40:47.551684Z","iopub.status.idle":"2025-08-18T10:41:02.935357Z","shell.execute_reply.started":"2025-08-18T10:40:47.551665Z","shell.execute_reply":"2025-08-18T10:41:02.934712Z"}},"outputs":[{"name":"stdout","text":"Deleting all contents in '/gpt-oss-20b'...\nDeleted file: /gpt-oss-20b/.cache/huggingface/download/config.json.metadata\nDeleted file: /gpt-oss-20b/.cache/huggingface/download/.gitattributes.lock\nDeleted file: /gpt-oss-20b/.cache/huggingface/download/config.json.lock\nDeleted file: /gpt-oss-20b/.cache/huggingface/download/generation_config.json.lock\nDeleted file: /gpt-oss-20b/.cache/huggingface/download/pytorch_model.bin.index.json.metadata\nDeleted file: /gpt-oss-20b/.cache/huggingface/download/pytorch_model.bin.index.json.lock\nDeleted file: /gpt-oss-20b/.cache/huggingface/download/LICENSE.metadata\nDeleted file: /gpt-oss-20b/.cache/huggingface/download/README.md.lock\nDeleted file: /gpt-oss-20b/.cache/huggingface/download/.gitattributes.metadata\nDeleted file: /gpt-oss-20b/.cache/huggingface/download/LICENSE.lock\nDeleted file: /gpt-oss-20b/.cache/huggingface/download/tokenizer.json.lock\nDeleted file: /gpt-oss-20b/.cache/huggingface/download/special_tokens_map.json.metadata\nDeleted file: /gpt-oss-20b/.cache/huggingface/download/tokenizer_config.json.lock\nDeleted file: /gpt-oss-20b/.cache/huggingface/download/tokenizer_config.json.metadata\nDeleted file: /gpt-oss-20b/.cache/huggingface/download/tokenizer.json.metadata\nDeleted file: /gpt-oss-20b/.cache/huggingface/download/special_tokens_map.json.lock\nDeleted file: /gpt-oss-20b/.cache/huggingface/download/generation_config.json.metadata\nDeleted file: /gpt-oss-20b/.cache/huggingface/download/README.md.metadata\nSuccessfully deleted folder: /gpt-oss-20b/.cache/huggingface/download\nDeleted file: /gpt-oss-20b/.cache/huggingface/.gitignore\nSuccessfully deleted folder: /gpt-oss-20b/.cache/huggingface\nSuccessfully deleted folder: /gpt-oss-20b/.cache\nDeleted file: /gpt-oss-20b/LICENSE\nDeleted file: /gpt-oss-20b/README.md\nDeleted file: /gpt-oss-20b/config.json\nDeleted file: /gpt-oss-20b/.gitattributes\nDeleted file: /gpt-oss-20b/tokenizer_config.json\nDeleted file: /gpt-oss-20b/generation_config.json\nDeleted file: /gpt-oss-20b/special_tokens_map.json\nDeleted file: /gpt-oss-20b/pytorch_model.bin.index.json\nDeleted file: /gpt-oss-20b/tokenizer.json\nSuccessfully deleted folder: /gpt-oss-20b\nCleanup complete!\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Step 2: Set up Hugging Face and Dropbox\nfrom huggingface_hub import snapshot_download\nimport dropbox\nimport os\n\n# Hugging Face model (replace if using a different model)\nmodel_id = \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"  # Example (GPT-OSS-20B may not exist; adjust)\nlocal_model_dir = \"./downloaded_model\"\n\n# Dropbox setup\n#DROPBOX_ACCESS_TOKEN = \"your_dropbox_access_token_here\"  # Replace with your token\nDROPBOX_UPLOAD_PATH = \"/gpt-oss-20b\"  # Target folder in Dropbox\n\n# Step 3: Download from Hugging Face Hub\nprint(\"Downloading model from Hugging Face...\")\nsnapshot_download(\n    repo_id=model_id,\n    local_dir=local_model_dir,\n    token=True,  # Required for gated models (if applicable)\n    ignore_patterns=[\"*.bin\", \"*.h5\"]  # Optional: Exclude large files if needed\n)\n\n# Step 4: Upload to Dropbox\ndef upload_to_dropbox(local_path, dropbox_path):\n    dbx = dropbox.Dropbox(DROPBOX_ACCESS_TOKEN)\n    for root, dirs, files in os.walk(local_path):\n        for file in files:\n            local_file_path = os.path.join(root, file)\n            relative_path = os.path.relpath(local_file_path, local_path)\n            dropbox_file_path = os.path.join(dropbox_path, relative_path)\n            with open(local_file_path, \"rb\") as f:\n                dbx.files_upload(f.read(), dropbox_file_path, mode=dropbox.files.WriteMode(\"overwrite\"))\n            print(f\"Uploaded: {dropbox_file_path}\")\n\nprint(\"Uploading to Dropbox...\")\nupload_to_dropbox(local_model_dir, DROPBOX_UPLOAD_PATH)\n\n# Step 5: Clean up (optional)\n!rm -rf $local_model_dir\nprint(\"Done! Model saved to Dropbox.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T10:22:08.354015Z","iopub.execute_input":"2025-08-18T10:22:08.354672Z","iopub.status.idle":"2025-08-18T10:22:09.941296Z","shell.execute_reply.started":"2025-08-18T10:22:08.354641Z","shell.execute_reply":"2025-08-18T10:22:09.940108Z"}},"outputs":[{"name":"stdout","text":"Downloading model from Hugging Face...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fad8113703db4c06b3c9f57be45e5ef7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aa1c0869a5f4b8aaded9e25a6578366"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceff78094e5740389c6654058917a433"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/303 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f172aba9f5c8414a91015b12141dbd42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60b7eb4e85964999847e8f9fd950c6a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d91492237ffa4d5dbe68f83cd1ab9be9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e642c9d6c9e49c182056c0a3baaa1f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e33ef3c39ee74b7bbcb0aa5ae01d32f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"LICENSE: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf353d9e12974b79a83a17acb491737e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d33b2583728c4195ae971c9af595c112"}},"metadata":{}},{"name":"stdout","text":"Uploading to Dropbox...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/simplejson/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, use_decimal, allow_nan, **kw)\u001b[0m\n\u001b[1;32m    513\u001b[0m             and not use_decimal and not allow_nan and not kw):\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/simplejson/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w, _PY3)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/simplejson/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx, _w, _PY3)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dropbox/dropbox_client.py\u001b[0m in \u001b[0;36mraise_dropbox_error_for_resp\u001b[0;34m(self, res)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'invalid_grant'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m                     \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x-dropbox-request-id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0;31m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRequestsJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mBadInputError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2224436233.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Uploading to Dropbox...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mupload_to_dropbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_model_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDROPBOX_UPLOAD_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Step 5: Clean up (optional)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/2224436233.py\u001b[0m in \u001b[0;36mupload_to_dropbox\u001b[0;34m(local_path, dropbox_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mdropbox_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropbox_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mdbx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles_upload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropbox_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWriteMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Uploaded: {dropbox_file_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dropbox/base.py\u001b[0m in \u001b[0;36mfiles_upload\u001b[0;34m(self, f, path, mode, autorename, client_modified, mute, property_groups, strict_conflict, content_hash)\u001b[0m\n\u001b[1;32m   3212\u001b[0m                               \u001b[0mstrict_conflict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3213\u001b[0m                               content_hash)\n\u001b[0;32m-> 3214\u001b[0;31m         r = self.request(\n\u001b[0m\u001b[1;32m   3215\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m             \u001b[0;34m'files'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dropbox/dropbox_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, route, namespace, request_arg, request_binary, timeout)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         res = self.request_json_string_with_retry(host,\n\u001b[0m\u001b[1;32m    327\u001b[0m                                                   \u001b[0mroute_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                                                   \u001b[0mroute_style\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dropbox/dropbox_client.py\u001b[0m in \u001b[0;36mrequest_json_string_with_retry\u001b[0;34m(self, host, route_name, route_style, request_json_arg, auth_type, request_binary, timeout)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Request to %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroute_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m                 return self.request_json_string(host,\n\u001b[0m\u001b[1;32m    477\u001b[0m                                                 \u001b[0mroute_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m                                                 \u001b[0mroute_style\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dropbox/dropbox_client.py\u001b[0m in \u001b[0;36mrequest_json_string\u001b[0;34m(self, host, func_name, route_style, request_json_arg, auth_type, request_binary, timeout)\u001b[0m\n\u001b[1;32m    593\u001b[0m                                \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                                )\n\u001b[0;32m--> 595\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_dropbox_error_for_resp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m         \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x-dropbox-request-id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m403\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m404\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m409\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dropbox/dropbox_client.py\u001b[0m in \u001b[0;36mraise_dropbox_error_for_resp\u001b[0;34m(self, res)\u001b[0m\n\u001b[1;32m    629\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mBadInputError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mBadInputError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m401\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             assert res.headers.get('content-type') == 'application/json', (\n","\u001b[0;31mBadInputError\u001b[0m: BadInputError('2af2b19239244e91bc4ad8781c832476', 'Error in call to API function \"files/upload\": Your app (ID: 5059235) is not permitted to access this endpoint because it does not have the required scope \\'files.content.write\\'. The owner of the app can enable the scope for the app using the Permissions tab on the App Console.')"],"ename":"BadInputError","evalue":"BadInputError('2af2b19239244e91bc4ad8781c832476', 'Error in call to API function \"files/upload\": Your app (ID: 5059235) is not permitted to access this endpoint because it does not have the required scope \\'files.content.write\\'. The owner of the app can enable the scope for the app using the Permissions tab on the App Console.')","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\n\nmodel_id = \"openai/gpt-oss-20b\"\n\nsnapshot_download(repo_id=model_id, local_dir=\"./gpt-oss-20b\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"HF_TOKEN = \"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\"\nDROPBOX_ACCESS_TOKEN=\"sl.u.AF5bmQnc57vpaG6t89Zc6ujRFL3QipS4Hhl6E4_7_JuKsNwyAyeehFYm_iE-i9j9K4Yn2d5j_96DKy2eDKnyMGjFu3EKIFLW52WrEzQZxA46xeMBU6Pd4L7U0wtd1t0ZcWeLXSFIzY18dxgI59uFZHUTUhy9YkiACoAOp4rt9_8Us_0IbD6XZXK4JK6NLpnaXDK576Y0PpvHR26toYYTlpHcZr_mmIQ6csSieUzZDF1shGM402uCdacPifbfzjklJmB9JOAZYM3zCSumNoPWyd6rx4YzHaOkg2yOVHCwwhD5dwQb46kJ-vYTtUdw4jhT7wi4jglIqp3p6L8rbNMQi4dgAs55j_heP-yiouyJIUoMjNnmf43kjBkaYUwOkUDqeb-4l2WWvUaTo_1l-_TNBOrjy43fyIGJRczs5_yH_-A3Oktl04YciK1EWnMkETcNFNReIlOyiH9yZgEGEn9gwScnb5mBdbpUNP9E6Y4QapqY9FYj8WN1hv15rKJ1NVUYgtYhzElx7kLhLiWte5kiGptuwlSmUEpJSt_DPto29IbbIgFsA8YRelMSmlRUnUQvFYGUhUzFHAbVuk_KhSk-eNTFt7dMzDgI98VEjh2AK_nQGQondGgd4LAOa4koQeOEjD3SrMbV_Q9hh4seL8FXbfBg74CGWQvThk9Xl0KSoAjciQ5bL3xfYb1mh2wQf2WiPSEOwitqAQRZGqFaQBWuv8GRTsqp_77s5Tl4uWBBr0Pi0gQjUvatXrOylgydFafZa4set7Fa-CTjYBKhBYZ0IYpR4e-FewDpcCqrO8_D66txBuwENa4sg77g29058S-CiJdeQlm4cOOEv4DhjT6FXRMEqiGalmtj0XIkeDRVQDKJzjTirsnq9AUpT24izL7-jyXrBLr3gc_LFCdAxzGAVTOLEEX7CWaeDIAco8IhI6wWnbjkhhJqdGzPboltCOCgpjCIc7W_h7LKeA3QMR5-ZQVXedmh70IHjaAJT7QO7hSfoom8dOFV5I5MGbGYxutEggwtQ2z3SobhrtsfTb1fVTxzdUk7i4aLHQkxQhvtin1WPr01f2d_hW3KJ59T7U_5eEzAo1M3mfW6alANPieO3zSZbRc9WVw8q3BLICkE34eFi-pwBzBNMShFs2CNYv4eP4wblXF2Y1ln3kVyGxLImAxNN6SLcL2aQJZsmgdt7jeKxkRZvrH89tzowrti0bVpx6fjXOpdZbTAInBznLrmWZARsvBAZywTB2iJP0lHzaO_Xi8s2kUb8jbDIl9fXObmqGQ\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T11:03:28.531473Z","iopub.execute_input":"2025-08-18T11:03:28.532006Z","iopub.status.idle":"2025-08-18T11:03:28.535918Z","shell.execute_reply.started":"2025-08-18T11:03:28.531979Z","shell.execute_reply":"2025-08-18T11:03:28.535112Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# HF -> Dropbox (server-side) using Dropbox \"save_url\" (no Kaggle disk usage)\n\nimport os, posixpath, time\nfrom tqdm import tqdm\nfrom huggingface_hub import HfApi, hf_hub_url\nimport dropbox\nfrom dropbox.exceptions import AuthError, ApiError\n\nMODEL_ID = \"openai/gpt-oss-20b\"\nDROPBOX_FOLDER = \"/p1-gpt-oss-20b\"   # must start with \"/\"\n\n# --- Secrets (Kaggle) ---\ndef get_secret(name):\n    try:\n        from kaggle_secrets import UserSecretsClient\n        return UserSecretsClient().get_secret(name)\n    except Exception:\n        return os.environ.get(name)\n\n#DROPBOX_ACCESS_TOKEN = get_secret(\"DROPBOX_ACCESS_TOKEN\")\nassert DROPBOX_ACCESS_TOKEN, \"Missing DROPBOX_ACCESS_TOKEN (add it in Kaggle Secrets).\"\n\n# --- Init clients ---\napi = HfApi()\ndbx = dropbox.Dropbox(DROPBOX_ACCESS_TOKEN, timeout=60)\n\n# Verify Dropbox\ntry:\n    acc = dbx.users_get_current_account()\n    print(f\"✅ Dropbox: {acc.name.display_name}\")\nexcept AuthError as e:\n    raise SystemExit(f\"Dropbox auth failed: {e}\")\n\n# Ensure folder tree exists (idempotent)\ndef ensure_folder(path: str):\n    path = path.rstrip(\"/\")\n    if not path or path == \"/\":\n        return\n    parts = [p for p in path.split(\"/\") if p]\n    cur = \"\"\n    for p in parts:\n        cur = \"/\" + (p if not cur else cur.strip(\"/\") + \"/\" + p)\n        try:\n            dbx.files_create_folder_v2(cur)\n        except ApiError:\n            pass  # already exists\n\nensure_folder(DROPBOX_FOLDER)\n\n# Optional filter to reduce what you mirror (None = everything)\n# ALLOW_SUFFIXES = {\".safetensors\", \".json\", \".md\", \".txt\", \".gitattributes\", \".jinja\"}\nALLOW_SUFFIXES = None\n\ndef want_file(relpath: str) -> bool:\n    if ALLOW_SUFFIXES is None:\n        return True\n    return any(relpath.endswith(ext) for ext in ALLOW_SUFFIXES)\n\n# List HF files (default branch)\nrepo_files = api.list_repo_files(MODEL_ID, repo_type=\"model\")\nprint(f\"Found {len(repo_files)} files in {MODEL_ID}.\")\n\ndef save_url_to_dropbox(dst_path: str, url: str, poll_interval=3, max_wait=36000):\n    \"\"\"\n    Use files_save_url so Dropbox downloads directly from the URL and poll until done.\n    Correctly check tags before using get_*() accessors.\n    \"\"\"\n    ensure_folder(posixpath.dirname(dst_path))\n\n    # Submit job\n    res = dbx.files_save_url(dst_path, url)\n    # res is SaveUrlResult: tag is either 'complete' or 'async_job_id'\n    if res.is_complete():\n        return res.get_complete()  # done instantly\n\n    job_id = res.get_async_job_id()\n    waited = 0\n    while True:\n        status = dbx.files_save_url_check_job_status(job_id)\n        # status is SaveUrlJobStatus: tag can be 'in_progress' | 'complete' | 'failed'\n        if status.is_complete():\n            return status.get_complete()\n        if status.is_failed():\n            # get_failed() returns SaveUrlError, surface the reason\n            raise RuntimeError(f\"Dropbox save_url failed for {dst_path}: {status.get_failed()}\")\n        time.sleep(poll_interval)\n        waited += poll_interval\n        if waited > max_wait:\n            raise TimeoutError(f\"Timed out saving {dst_path} from {url}\")\n\nerrors = []\nfor rel in tqdm(repo_files, desc=\"Submitting Dropbox save jobs\"):\n    if not want_file(rel):\n        continue\n    url = hf_hub_url(MODEL_ID, filename=rel, repo_type=\"model\", revision=\"main\")\n    dst = posixpath.join(DROPBOX_FOLDER, rel)\n    try:\n        save_url_to_dropbox(dst, url)\n    except Exception as e:\n        errors.append((rel, str(e)))\n\nif errors:\n    print(\"\\nSome files failed:\")\n    for rel, msg in errors:\n        print(f\"  - {rel}: {msg}\")\nelse:\n    print(f\"\\n✅ All files saved directly to Dropbox at {DROPBOX_FOLDER} (no local storage used).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T11:13:19.282676Z","iopub.execute_input":"2025-08-18T11:13:19.283281Z","iopub.status.idle":"2025-08-18T11:37:41.988599Z","shell.execute_reply.started":"2025-08-18T11:13:19.283259Z","shell.execute_reply":"2025-08-18T11:37:41.987813Z"}},"outputs":[{"name":"stdout","text":"✅ Dropbox: M Waqas\nFound 18 files in openai/gpt-oss-20b.\n","output_type":"stream"},{"name":"stderr","text":"Submitting Dropbox save jobs:  39%|███▉      | 7/18 [24:22<38:17, 208.88s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/585998621.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposixpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDROPBOX_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0msave_url_to_dropbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/585998621.py\u001b[0m in \u001b[0;36msave_url_to_dropbox\u001b[0;34m(dst_path, url, poll_interval, max_wait)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;31m# get_failed() returns SaveUrlError, surface the reason\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dropbox save_url failed for {dst_path}: {status.get_failed()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mwaited\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpoll_interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwaited\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_wait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nimport os, posixpath, math, time, requests\nfrom tqdm import tqdm\nfrom huggingface_hub import HfApi, hf_hub_url\nimport dropbox\nfrom dropbox.exceptions import ApiError, AuthError\n\n# ----- Config -----\nMODEL_ID = \"openai/gpt-oss-20b\"\nDROPBOX_FOLDER = \"/p1-gpt-oss-20b\"      # must start with \"/\"\nCHUNK = 8 * 1024 * 1024                 # 8 MB (safe for Dropbox upload sessions)\nMAX_RETRIES = 5\nTIMEOUT = (10, 120)                     # (connect, read) seconds\n\ndef get_secret(name):\n    try:\n        from kaggle_secrets import UserSecretsClient\n        return UserSecretsClient().get_secret(name)\n    except Exception:\n        return os.environ.get(name)\n\n#HF_TOKEN = get_secret(\"HF_TOKEN\") or get_secret(\"HUGGINGFACE_TOKEN\")\n#DBX_TOKEN = get_secret(\"DROPBOX_ACCESS_TOKEN\")\nDBX_TOKEN = DROPBOX_ACCESS_TOKEN\nassert DBX_TOKEN, \"Missing DROPBOX_ACCESS_TOKEN (Kaggle Secrets).\"\n\napi = HfApi()\ndbx = dropbox.Dropbox(DBX_TOKEN, timeout=300)\n\n# Verify Dropbox\ntry:\n    acc = dbx.users_get_current_account()\n    print(f\"✅ Dropbox: {acc.name.display_name}\")\nexcept AuthError as e:\n    raise SystemExit(f\"Dropbox auth failed: {e}\")\n\n# Ensure folder tree exists (idempotent)\ndef ensure_folder(path: str):\n    path = path.rstrip(\"/\")\n    if not path or path == \"/\":\n        return\n    parts = [p for p in path.split(\"/\") if p]\n    cur = \"\"\n    for p in parts:\n        cur = \"/\" + (p if not cur else cur.strip(\"/\") + \"/\" + p)\n        try:\n            dbx.files_create_folder_v2(cur)\n        except ApiError:\n            pass\n\nensure_folder(DROPBOX_FOLDER)\n\n# --- Select files (same filters you used) ---\nINCLUDE_SUFFIXES = {\".safetensors\", \".json\", \".md\", \".txt\", \".gitattributes\", \".jinja\"}\nEXCLUDE_PREFIXES = (\"original/\", \"metal/\")\n\ndef want_file(relpath: str) -> bool:\n    if relpath.startswith(EXCLUDE_PREFIXES):\n        return False\n    return any(relpath.endswith(ext) for ext in INCLUDE_SUFFIXES)\n\nrepo_files = api.list_repo_files(MODEL_ID, repo_type=\"model\")\nwanted = [p for p in repo_files if want_file(p)]\nprint(f\"Found {len(repo_files)} files; will transfer {len(wanted)} essential files.\")\n\n# --- Helpers ---\nsession = requests.Session()\nif HF_TOKEN:\n    session.headers[\"Authorization\"] = f\"Bearer {HF_TOKEN}\"\n\ndef file_exists_in_dropbox(dst_path: str) -> bool:\n    try:\n        dbx.files_get_metadata(dst_path)\n        return True\n    except ApiError as e:\n        # not_found -> upload needed\n        return False\n\ndef stream_from_hf(relpath: str):\n    url = hf_hub_url(MODEL_ID, filename=relpath, repo_type=\"model\", revision=\"main\")\n    # stream with retries\n    for attempt in range(1, MAX_RETRIES+1):\n        try:\n            r = session.get(url, stream=True, timeout=TIMEOUT)\n            if r.status_code == 302:\n                # follow redirect manually with same headers\n                redir = r.headers.get(\"Location\")\n                r.close()\n                r = session.get(redir, stream=True, timeout=TIMEOUT)\n            r.raise_for_status()\n            size = int(r.headers.get(\"Content-Length\", \"0\"))\n            return r, size\n        except Exception as e:\n            if attempt == MAX_RETRIES:\n                raise\n            time.sleep(1.5 * attempt)\n\ndef upload_stream_to_dropbox(dst_path: str, stream, total_size: int):\n    import sys\n    ensure_folder(posixpath.dirname(dst_path))\n\n    # Read first chunk to start a Dropbox upload session\n    data = stream.raw.read(CHUNK, decode_content=True)\n    if not data:\n        # empty file\n        dbx.files_upload(b\"\", dst_path, mode=dropbox.files.WriteMode(\"overwrite\"))\n        tqdm.write(f\"✔ {posixpath.basename(dst_path)} (empty)\")\n        return\n\n    start_res = dbx.files_upload_session_start(data)\n    cursor = dropbox.files.UploadSessionCursor(session_id=start_res.session_id, offset=len(data))\n    commit = dropbox.files.CommitInfo(path=dst_path, mode=dropbox.files.WriteMode(\"overwrite\"))\n\n    # Show a single-line progress bar only for files >= 1MB\n    show_bar = (total_size or 0) >= (1 * 1024 * 1024)\n    pbar = None\n    if show_bar:\n        pbar = tqdm(\n            total=total_size if total_size else None,\n            unit=\"B\",\n            unit_scale=True,\n            unit_divisor=1024,\n            desc=posixpath.basename(dst_path),\n            dynamic_ncols=True,\n            mininterval=1.0,\n            maxinterval=5.0,\n            smoothing=0.1,\n            leave=False,           # <-- don't keep the bar after completion\n            position=0,\n            file=sys.stdout,       # <-- keep it on stdout (single line)\n        )\n        pbar.update(len(data))\n\n    def _update_bar(n):\n        if pbar:\n            pbar.update(n)\n\n    # Upload remaining chunks with retry\n    while True:\n        chunk = stream.raw.read(CHUNK, decode_content=True)\n        if not chunk:\n            break\n        for attempt in range(1, MAX_RETRIES + 1):\n            try:\n                # If we know the size, only finish on the last chunk\n                if (total_size and (cursor.offset + len(chunk)) < total_size) or (total_size == 0):\n                    dbx.files_upload_session_append_v2(chunk, cursor)\n                    cursor.offset += len(chunk)\n                else:\n                    dbx.files_upload_session_finish(chunk, cursor, commit)\n                    cursor.offset += len(chunk)\n                break\n            except Exception as e:\n                if attempt == MAX_RETRIES:\n                    if pbar:\n                        pbar.close()\n                    raise\n                time.sleep(1.5 * attempt)\n        _update_bar(len(chunk))\n\n    # Safety: if we didn’t hit finish above (e.g., unknown total_size), finalize now\n    if total_size == 0 or (total_size and cursor.offset < total_size):\n        dbx.files_upload_session_finish(b\"\", cursor, commit)\n\n    if pbar:\n        pbar.close()\n\n    # One compact line per completed file\n    size_str = f\"{(total_size or cursor.offset) / (1024**3):.2f} GB\" if (total_size or cursor.offset) > 2**20 else f\"{(total_size or cursor.offset)/1024:.0f} KB\"\n    tqdm.write(f\"✔ {posixpath.basename(dst_path)} ({size_str})\")\n\n\n\nerrors = {}\ndone = 0\nfor rel in wanted:\n    dst = posixpath.join(DROPBOX_FOLDER, rel)\n    try:\n        if file_exists_in_dropbox(dst):\n            print(f\"↪️  Skip (exists): {dst}\")\n            done += 1\n            continue\n        stream, size = stream_from_hf(rel)\n        upload_stream_to_dropbox(dst, stream, size)\n        stream.close()\n        done += 1\n    except Exception as e:\n        errors[rel] = str(e)\n        print(f\"❌ {rel}: {e}\")\n\nprint(f\"\\nDone: {done} | Failed: {len(errors)} | Folder: {DROPBOX_FOLDER}\")\nif errors:\n    for k,v in errors.items():\n        print(f\"  - {k}: {v}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T11:55:48.741755Z","iopub.execute_input":"2025-08-18T11:55:48.742027Z","iopub.status.idle":"2025-08-18T12:30:12.769062Z","shell.execute_reply.started":"2025-08-18T11:55:48.742006Z","shell.execute_reply":"2025-08-18T12:30:12.768527Z"}},"outputs":[{"name":"stdout","text":"✅ Dropbox: M Waqas\nFound 18 files; will transfer 12 essential files.\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \nPolling save_url jobs:  75%|███████▌  | 9/12 [16:58<00:10,  3.49s/it]                \nmodel-00002-of-00002.safetensors:  11%|█         | 445M/4.17G [06:59<06:51, 9.06MB/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"✔ .gitattributes (2 KB)\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \nPolling save_url jobs:  75%|███████▌  | 9/12 [17:00<00:10,  3.49s/it]                \nmodel-00002-of-00002.safetensors:  11%|█         | 445M/4.17G [07:01<06:51, 9.06MB/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"✔ README.md (7 KB)\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \nPolling save_url jobs:  75%|███████▌  | 9/12 [17:02<00:10,  3.49s/it]                \nmodel-00002-of-00002.safetensors:  11%|█         | 445M/4.17G [07:03<06:51, 9.06MB/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"✔ chat_template.jinja (16 KB)\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \nPolling save_url jobs:  75%|███████▌  | 9/12 [17:03<00:10,  3.49s/it]                \nmodel-00002-of-00002.safetensors:  11%|█         | 445M/4.17G [07:05<06:51, 9.06MB/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"✔ config.json (2 KB)\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \nPolling save_url jobs:  75%|███████▌  | 9/12 [17:04<00:10,  3.49s/it]                \nmodel-00002-of-00002.safetensors:  11%|█         | 445M/4.17G [07:06<06:51, 9.06MB/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"✔ generation_config.json (0 KB)\n                                                                                       ","output_type":"stream"},{"name":"stderr","text":"                                                                     \nPolling save_url jobs:  75%|███████▌  | 9/12 [30:32<00:10,  3.49s/it]                \nmodel-00002-of-00002.safetensors:  11%|█         | 445M/4.17G [20:34<06:51, 9.06MB/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"✔ model-00000-of-00002.safetensors (4.46 GB)\n                                                                                      ","output_type":"stream"},{"name":"stderr","text":"                                                                     \nPolling save_url jobs:  75%|███████▌  | 9/12 [38:33<00:10,  3.49s/it]                \nmodel-00002-of-00002.safetensors:  11%|█         | 445M/4.17G [28:34<06:51, 9.06MB/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"✔ model-00001-of-00002.safetensors (4.47 GB)\n                                                                                      ","output_type":"stream"},{"name":"stderr","text":"                                                                     \nPolling save_url jobs:  75%|███████▌  | 9/12 [51:10<00:10,  3.49s/it]                \nmodel-00002-of-00002.safetensors:  11%|█         | 445M/4.17G [41:11<06:51, 9.06MB/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"✔ model-00002-of-00002.safetensors (3.88 GB)\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \nPolling save_url jobs:  75%|███████▌  | 9/12 [51:12<00:10,  3.49s/it]                \nmodel-00002-of-00002.safetensors:  11%|█         | 445M/4.17G [41:13<06:51, 9.06MB/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"✔ model.safetensors.index.json (36 KB)\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \nPolling save_url jobs:  75%|███████▌  | 9/12 [51:13<00:10,  3.49s/it]                \nmodel-00002-of-00002.safetensors:  11%|█         | 445M/4.17G [41:14<06:51, 9.06MB/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"✔ special_tokens_map.json (0 KB)\n                                                                    ","output_type":"stream"},{"name":"stderr","text":"                                                                     \nPolling save_url jobs:  75%|███████▌  | 9/12 [51:17<00:10,  3.49s/it]                \nmodel-00002-of-00002.safetensors:  11%|█         | 445M/4.17G [41:19<06:51, 9.06MB/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"✔ tokenizer.json (0.03 GB)\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \nPolling save_url jobs:  75%|███████▌  | 9/12 [51:19<00:10,  3.49s/it]                \nmodel-00002-of-00002.safetensors:  11%|█         | 445M/4.17G [41:21<06:51, 9.06MB/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"✔ tokenizer_config.json (4 KB)\n\nDone: 12 | Failed: 0 | Folder: /p1-gpt-oss-20b\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# pip install dropbox\nimport os, dropbox\nfrom humanize import naturalsize\n\nDBX_TOKEN = DROPBOX_ACCESS_TOKEN\nFOLDER = \"/p1-gpt-oss-20b\"\n\ndbx = dropbox.Dropbox(DBX_TOKEN)\nentries = []\nres = dbx.files_list_folder(FOLDER)\nentries += res.entries\nwhile res.has_more:\n    res = dbx.files_list_folder_continue(res.cursor)\n    entries += res.entries\n\nfiles = [e for e in entries if isinstance(e, dropbox.files.FileMetadata)]\nfor f in sorted(files, key=lambda x: x.name):\n    print(f\"{f.name}\\t{naturalsize(f.size)}\")\n\nprint(\"Total:\", naturalsize(sum(f.size for f in files)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T12:59:10.618438Z","iopub.execute_input":"2025-08-18T12:59:10.618992Z","iopub.status.idle":"2025-08-18T12:59:10.872563Z","shell.execute_reply.started":"2025-08-18T12:59:10.618967Z","shell.execute_reply":"2025-08-18T12:59:10.871854Z"}},"outputs":[{"name":"stdout","text":".gitattributes\t1.6 kB\nREADME.md\t6.8 kB\nchat_template.jinja\t16.7 kB\nconfig.json\t1.8 kB\nmodel-00000-of-00002.safetensors\t4.8 GB\nmodel-00001-of-00002.safetensors\t4.8 GB\nmodel-00002-of-00002.safetensors\t4.2 GB\nmodel.safetensors.index.json\t36.4 kB\ntokenizer.json\t27.9 MB\ntokenizer_config.json\t4.2 kB\nTotal: 13.8 GB\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###New code###\n# Clean HF -> Dropbox (upload sessions, single-line tqdm, no noisy outputs)\n!pip -q install -U huggingface_hub dropbox tqdm requests >/dev/null\n\nimport os, posixpath, time, requests, sys\nfrom contextlib import redirect_stderr\nfrom tqdm import tqdm\nfrom huggingface_hub import HfApi, hf_hub_url\nimport dropbox\nfrom dropbox.exceptions import ApiError, AuthError\n\n# ---------------- Config ----------------\nMODEL_ID = \"openai/gpt-oss-20b\"\nDROPBOX_FOLDER = \"/p1-gpt-oss-20b\"          # must start with \"/\"\nCHUNK = 8 * 1024 * 1024                     # 8 MB chunk for Dropbox upload sessions\nMAX_RETRIES = 5\nTIMEOUT = (10, 120)                         # (connect, read) seconds\nSHOW_BAR_MIN_BYTES = 1 * 1024 * 1024        # only show tqdm for files >= 1MB\n\ndef get_secret(name):\n    try:\n        from kaggle_secrets import UserSecretsClient\n        return UserSecretsClient().get_secret(name)\n    except Exception:\n        return os.environ.get(name)\n\nHF_TOKEN = get_secret(\"HF_TOKEN\") or get_secret(\"HUGGINGFACE_TOKEN\")  # optional\nDBX_TOKEN = get_secret(\"DROPBOX_ACCESS_TOKEN\")\nassert DBX_TOKEN, \"Missing DROPBOX_ACCESS_TOKEN (add it in Kaggle Secrets).\"\n\n# ---------------- Clients ----------------\napi = HfApi()\ndbx = dropbox.Dropbox(DBX_TOKEN, timeout=300)\n\n# Verify Dropbox\ntry:\n    acc = dbx.users_get_current_account()\n    print(f\"✅ Dropbox: {acc.name.display_name}\")\nexcept AuthError as e:\n    raise SystemExit(f\"Dropbox auth failed: {e}\")\n\n# Ensure folder tree exists (idempotent)\ndef ensure_folder(path: str):\n    path = path.rstrip(\"/\")\n    if not path or path == \"/\":\n        return\n    parts = [p for p in path.split(\"/\") if p]\n    cur = \"\"\n    for p in parts:\n        cur = \"/\" + (p if not cur else cur.strip(\"/\") + \"/\" + p)\n        try:\n            dbx.files_create_folder_v2(cur)\n        except ApiError:\n            pass\n\nensure_folder(DROPBOX_FOLDER)\n\n# ---------------- Select files ----------------\nINCLUDE_SUFFIXES = {\".safetensors\", \".json\", \".md\", \".txt\", \".gitattributes\", \".jinja\"}\nEXCLUDE_PREFIXES = (\"original/\", \"metal/\")\n\ndef want_file(relpath: str) -> bool:\n    if relpath.startswith(EXCLUDE_PREFIXES):\n        return False\n    return any(relpath.endswith(ext) for ext in INCLUDE_SUFFIXES)\n\nrepo_files = api.list_repo_files(MODEL_ID, repo_type=\"model\")\nwanted = [p for p in repo_files if want_file(p)]\nprint(f\"Found {len(repo_files)} files; will transfer {len(wanted)} essential files.\")\n\n# ---------------- HTTP session (HF) ----------------\nsession = requests.Session()\nif HF_TOKEN:\n    session.headers[\"Authorization\"] = f\"Bearer {HF_TOKEN}\"\n\n# ---------------- Helpers ----------------\ndef file_exists_in_dropbox(dst_path: str) -> bool:\n    try:\n        dbx.files_get_metadata(dst_path)\n        return True\n    except ApiError:\n        return False\n\ndef stream_from_hf(relpath: str):\n    url = hf_hub_url(MODEL_ID, filename=relpath, repo_type=\"model\", revision=\"main\")\n    for attempt in range(1, MAX_RETRIES + 1):\n        try:\n            r = session.get(url, stream=True, timeout=TIMEOUT, allow_redirects=False)\n            if r.status_code in (301, 302, 303, 307, 308):\n                redir = r.headers.get(\"Location\")\n                r.close()\n                r = session.get(redir, stream=True, timeout=TIMEOUT)\n            r.raise_for_status()\n            size = int(r.headers.get(\"Content-Length\", \"0\")) if r.headers.get(\"Content-Length\") else 0\n            return r, size\n        except Exception:\n            if attempt == MAX_RETRIES:\n                raise\n            time.sleep(1.5 * attempt)\n\ndef upload_stream_to_dropbox(dst_path: str, stream, total_size: int):\n    ensure_folder(posixpath.dirname(dst_path))\n\n    # Start session with first chunk\n    first = stream.raw.read(CHUNK, decode_content=True)\n    if not first:\n        dbx.files_upload(b\"\", dst_path, mode=dropbox.files.WriteMode(\"overwrite\"))\n        tqdm.write(f\"✔ {posixpath.basename(dst_path)} (empty)\")\n        return\n\n    start_res = dbx.files_upload_session_start(first)\n    cursor = dropbox.files.UploadSessionCursor(session_id=start_res.session_id, offset=len(first))\n    commit = dropbox.files.CommitInfo(path=dst_path, mode=dropbox.files.WriteMode(\"overwrite\"))\n\n    show_bar = (total_size or 0) >= SHOW_BAR_MIN_BYTES\n    pbar = None\n    if show_bar:\n        pbar = tqdm(\n            total=total_size if total_size else None,\n            unit=\"B\", unit_scale=True, unit_divisor=1024,\n            desc=posixpath.basename(dst_path),\n            dynamic_ncols=True, mininterval=1.0, maxinterval=5.0, smoothing=0.1,\n            leave=False, position=0, file=sys.stdout,\n        )\n        pbar.update(len(first))\n\n    def bump(n): \n        if pbar: pbar.update(n)\n\n    while True:\n        chunk = stream.raw.read(CHUNK, decode_content=True)\n        if not chunk:\n            break\n        for attempt in range(1, MAX_RETRIES + 1):\n            try:\n                # If size is known, finish only on last chunk\n                if (total_size and (cursor.offset + len(chunk)) < total_size) or (total_size == 0):\n                    dbx.files_upload_session_append_v2(chunk, cursor)\n                    cursor.offset += len(chunk)\n                else:\n                    dbx.files_upload_session_finish(chunk, cursor, commit)\n                    cursor.offset += len(chunk)\n                break\n            except Exception:\n                if attempt == MAX_RETRIES:\n                    if pbar: pbar.close()\n                    raise\n                time.sleep(1.5 * attempt)\n        bump(len(chunk))\n\n    # Safety finalize (unknown or mismatched size)\n    if total_size == 0 or (total_size and cursor.offset < total_size):\n        dbx.files_upload_session_finish(b\"\", cursor, commit)\n\n    if pbar:\n        pbar.close()\n\n    final_sz = total_size or cursor.offset\n    size_str = f\"{final_sz / (1024**3):.2f} GB\" if final_sz >= (2**20) else f\"{final_sz/1024:.0f} KB\"\n    tqdm.write(f\"✔ {posixpath.basename(dst_path)} ({size_str})\")\n\n# ---------------- Transfer loop (stderr muted) ----------------\nerrors = {}\ndone = 0\n\nwith redirect_stderr(open(os.devnull, \"w\")):  # silence any stray tqdm on stderr\n    for rel in wanted:\n        dst = posixpath.join(DROPBOX_FOLDER, rel)\n        try:\n            if file_exists_in_dropbox(dst):\n                print(f\"↪️  Skip (exists): {dst}\")\n                done += 1\n                continue\n            stream, size = stream_from_hf(rel)\n            upload_stream_to_dropbox(dst, stream, size)\n            stream.close()\n            done += 1\n        except Exception as e:\n            errors[rel] = str(e)\n            print(f\"❌ {rel}: {e}\")\n\nprint(f\"Done: {done} | Failed: {len(errors)} | Folder: {DROPBOX_FOLDER}\")\nif errors:\n    for k, v in errors.items():\n        print(f\"  - {k}: {v}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}