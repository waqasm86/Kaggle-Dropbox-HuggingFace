{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install -U llama-cpp-python > /dev/null","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-08-21T10:39:43.077964Z","iopub.execute_input":"2025-08-21T10:39:43.078514Z","iopub.status.idle":"2025-08-21T10:41:45.277304Z","shell.execute_reply.started":"2025-08-21T10:39:43.078495Z","shell.execute_reply":"2025-08-21T10:41:45.276048Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Local Inference on GPU \nModel page: https://huggingface.co/ggml-org/gemma-3-1b-it-GGUF\n\n‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/ggml-org/gemma-3-1b-it-GGUF)\n\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"from llama_cpp import Llama\n\n# Initialize the model with the CORRECT filename\nllm = Llama.from_pretrained(\n    repo_id=\"ggml-org/gemma-3-1b-it-GGUF\",\n    filename=\"gemma-3-1b-it-Q4_K_M.gguf\",  # <-- This is the crucial fix\n    n_gpu_layers=-1,  # Offload all layers to the GPU(s) for maximum speed\n    n_ctx=4096,       # Context window size\n    verbose=False     # Set to True if you want to see details during loading\n)\n\nprint(\"Model loaded successfully!\")","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-08-21T10:44:33.708680Z","iopub.execute_input":"2025-08-21T10:44:33.709372Z","iopub.status.idle":"2025-08-21T10:44:38.436800Z","shell.execute_reply.started":"2025-08-21T10:44:33.709346Z","shell.execute_reply":"2025-08-21T10:44:38.436152Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"./gemma-3-1b-it-Q4_K_M.gguf:   0%|          | 0.00/806M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57538efad4e747b8a68d870cf30a60bf"}},"metadata":{}},{"name":"stderr","text":"llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\nllama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Create a prompt\nprompt = \"What is the capital of France? Explain in one sentence.\"\n\n# Generate a response\noutput = llm(\n    prompt,\n    max_tokens=256,  # Maximum number of tokens to generate\n    stop=[\"\\n\"],     # Stop generating at a newline (optional)\n    echo=True,       # Echo the prompt in the output\n    temperature=0.7  # Controls creativity (0 = deterministic, >1 = more creative)\n)\n\n# Print the result\nprint(output['choices'][0]['text'])","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-08-21T10:48:27.841733Z","iopub.execute_input":"2025-08-21T10:48:27.842013Z","iopub.status.idle":"2025-08-21T10:48:28.104664Z","shell.execute_reply.started":"2025-08-21T10:48:27.841995Z","shell.execute_reply":"2025-08-21T10:48:28.104055Z"}},"outputs":[{"name":"stdout","text":"What is the capital of France? Explain in one sentence.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install huggingface_hub > /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T10:43:49.240295Z","iopub.execute_input":"2025-08-21T10:43:49.240947Z","iopub.status.idle":"2025-08-21T10:43:52.292495Z","shell.execute_reply.started":"2025-08-21T10:43:49.240925Z","shell.execute_reply":"2025-08-21T10:43:52.291648Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install openai > /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T10:43:54.868584Z","iopub.execute_input":"2025-08-21T10:43:54.868876Z","iopub.status.idle":"2025-08-21T10:43:57.875726Z","shell.execute_reply.started":"2025-08-21T10:43:54.868846Z","shell.execute_reply":"2025-08-21T10:43:57.874887Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from huggingface_hub import login, logout, whoami","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T10:44:00.583812Z","iopub.execute_input":"2025-08-21T10:44:00.584097Z","iopub.status.idle":"2025-08-21T10:44:00.593255Z","shell.execute_reply.started":"2025-08-21T10:44:00.584069Z","shell.execute_reply":"2025-08-21T10:44:00.592632Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nHF_TOKEN = UserSecretsClient().get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T10:44:15.215284Z","iopub.execute_input":"2025-08-21T10:44:15.215571Z","iopub.status.idle":"2025-08-21T10:44:15.333844Z","shell.execute_reply.started":"2025-08-21T10:44:15.215548Z","shell.execute_reply":"2025-08-21T10:44:15.333114Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"login(token=HF_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T10:44:17.508322Z","iopub.execute_input":"2025-08-21T10:44:17.508857Z","iopub.status.idle":"2025-08-21T10:44:17.592842Z","shell.execute_reply.started":"2025-08-21T10:44:17.508833Z","shell.execute_reply":"2025-08-21T10:44:17.592159Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}