{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -q -y transformers datasets accelerate > /dev/null\n!pip install -q transformers datasets accelerate --upgrade > /dev/null\nimport transformers, datasets, accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T11:16:41.815278Z","iopub.execute_input":"2025-08-28T11:16:41.815560Z","iopub.status.idle":"2025-08-28T11:16:54.334515Z","shell.execute_reply.started":"2025-08-28T11:16:41.815532Z","shell.execute_reply":"2025-08-28T11:16:54.333598Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(f\"transformers version: {transformers.__version__}\")\nprint(f\"accelerate version: {accelerate.__version__}\")\nprint(f\"datasets version: {datasets.__version__}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T11:17:23.909120Z","iopub.execute_input":"2025-08-28T11:17:23.909420Z","iopub.status.idle":"2025-08-28T11:17:23.914325Z","shell.execute_reply.started":"2025-08-28T11:17:23.909392Z","shell.execute_reply":"2025-08-28T11:17:23.913576Z"}},"outputs":[{"name":"stdout","text":"transformers version: 4.55.4\naccelerate version: 1.10.1\ndatasets version: 4.0.0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# First, install required packages\n!pip install -qU gradio langfuse plotly sentencepiece protobuf --upgrade > /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T11:18:57.763776Z","iopub.execute_input":"2025-08-28T11:18:57.764657Z","iopub.status.idle":"2025-08-28T11:19:34.808179Z","shell.execute_reply.started":"2025-08-28T11:18:57.764631Z","shell.execute_reply":"2025-08-28T11:19:34.807173Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.32.0 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.32.0 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.32.0 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nimport gradio as gr\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport pandas as pd\nimport numpy as np\nimport random\nfrom datetime import datetime, timedelta\nimport langfuse\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Initialize Langfuse (replace with your actual credentials)\nlangfuse_client = langfuse.Langfuse(\n    public_key=\"pk-lf-12345678-1234-1234-1234-123456789012\",\n    secret_key=\"sk-lf-12345678-1234-1234-1234-123456789012\",\n    host=\"https://cloud.langfuse.com\"\n)\n\n# Load a lightweight model that can run on T4 GPU\nMODEL_NAME = \"microsoft/DialoGPT-small\"  # Very lightweight model\n\nclass LightweightLLM:\n    def __init__(self):\n        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(self.device)\n        self.trace = None\n        \n    def generate_response(self, prompt: str, max_length: int = 100):\n        \"\"\"Generate response with Langfuse observability\"\"\"\n        # Start a new trace\n        trace = langfuse_client.trace(\n            name=\"llm-generation\",\n            user_id=\"demo-user\",\n            metadata={\"model\": MODEL_NAME, \"device\": str(self.device)}\n        )\n        \n        generation = trace.generation(\n            name=\"chat-completion\",\n            model=MODEL_NAME,\n            model_parameters={\"max_length\": max_length, \"temperature\": 0.7}\n        )\n        \n        try:\n            inputs = self.tokenizer.encode(prompt + self.tokenizer.eos_token, return_tensors=\"pt\")\n            inputs = inputs.to(self.device)\n            \n            # Generate response\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    inputs,\n                    max_length=max_length,\n                    pad_token_id=self.tokenizer.eos_token_id,\n                    temperature=0.7,\n                    do_sample=True,\n                    top_p=0.9\n                )\n            \n            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            response = response.replace(prompt, \"\").strip()\n            \n            # Update generation with output\n            generation.end(\n                output=response,\n                usage={\"input_tokens\": len(inputs[0]), \"output_tokens\": len(outputs[0])}\n            )\n            \n            # Log quality score\n            trace.score(\n                name=\"response_quality\",\n                value=random.uniform(0.7, 0.95),  # Simulated quality score\n                comment=\"Automated quality assessment\"\n            )\n            \n            return response\n            \n        except Exception as e:\n            generation.end(\n                output=f\"Error: {str(e)}\",\n                status_message=str(e)\n            )\n            \n            trace.score(\n                name=\"error\",\n                value=0.0,\n                comment=f\"Error: {str(e)}\"\n            )\n            return f\"Error generating response: {str(e)}\"\n\n# Initialize the model\nllm = LightweightLLM()\n\n# Mock data for demonstration\ndef generate_mock_metrics():\n    \"\"\"Generate mock observability metrics\"\"\"\n    dates = [datetime.now() - timedelta(hours=i) for i in range(24)]\n    \n    return {\n        'latency': [random.uniform(0.1, 2.0) for _ in range(24)],\n        'throughput': [random.randint(50, 200) for _ in range(24)],\n        'error_rate': [random.uniform(0.01, 0.1) for _ in range(24)],\n        'quality_score': [random.uniform(0.6, 0.95) for _ in range(24)],\n        'dates': dates,\n        'token_usage': [random.randint(100, 500) for _ in range(24)]\n    }\n\ndef create_latency_chart():\n    \"\"\"Create latency visualization\"\"\"\n    metrics = generate_mock_metrics()\n    \n    fig = px.line(\n        x=metrics['dates'],\n        y=metrics['latency'],\n        title='Response Latency (seconds)',\n        labels={'x': 'Time', 'y': 'Latency (s)'}\n    )\n    fig.update_traces(line=dict(color='blue'))\n    return fig\n\ndef create_throughput_chart():\n    \"\"\"Create throughput visualization\"\"\"\n    metrics = generate_mock_metrics()\n    \n    fig = px.bar(\n        x=metrics['dates'],\n        y=metrics['throughput'],\n        title='Requests Throughput',\n        labels={'x': 'Time', 'y': 'Requests per hour'}\n    )\n    fig.update_traces(marker_color='green')\n    return fig\n\ndef create_quality_chart():\n    \"\"\"Create quality score visualization\"\"\"\n    metrics = generate_mock_metrics()\n    \n    fig = px.scatter(\n        x=metrics['dates'],\n        y=metrics['quality_score'],\n        title='Response Quality Scores',\n        labels={'x': 'Time', 'y': 'Quality Score (0-1)'}\n    )\n    fig.update_traces(marker=dict(color='orange', size=10))\n    return fig\n\ndef create_comprehensive_dashboard():\n    \"\"\"Create a comprehensive dashboard with multiple metrics\"\"\"\n    metrics = generate_mock_metrics()\n    \n    fig = make_subplots(\n        rows=3, cols=2,\n        subplot_titles=('Latency', 'Throughput', 'Error Rate', 'Quality Score', 'Token Usage', 'Summary'),\n        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n    )\n    \n    # Latency\n    fig.add_trace(\n        go.Scatter(x=metrics['dates'], y=metrics['latency'], name=\"Latency\", line=dict(color='blue')),\n        row=1, col=1\n    )\n    \n    # Throughput\n    fig.add_trace(\n        go.Bar(x=metrics['dates'], y=metrics['throughput'], name=\"Throughput\", marker_color='green'),\n        row=1, col=2\n    )\n    \n    # Error Rate\n    fig.add_trace(\n        go.Scatter(x=metrics['dates'], y=metrics['error_rate'], name=\"Error Rate\", line=dict(color='red')),\n        row=2, col=1\n    )\n    \n    # Quality Score\n    fig.add_trace(\n        go.Scatter(x=metrics['dates'], y=metrics['quality_score'], name=\"Quality\", line=dict(color='orange')),\n        row=2, col=2\n    )\n    \n    # Token Usage\n    fig.add_trace(\n        go.Bar(x=metrics['dates'], y=metrics['token_usage'], name=\"Tokens\", marker_color='purple'),\n        row=3, col=1\n    )\n    \n    # Summary stats\n    summary_stats = [\n        f\"Avg Latency: {np.mean(metrics['latency']):.2f}s\",\n        f\"Avg Throughput: {np.mean(metrics['throughput']):.0f}/hr\",\n        f\"Avg Error Rate: {np.mean(metrics['error_rate']):.2%}\",\n        f\"Avg Quality: {np.mean(metrics['quality_score']):.2f}\"\n    ]\n    \n    fig.add_trace(\n        go.Scatter(\n            x=[0, 1, 2, 3],\n            y=[0, 0, 0, 0],\n            text=summary_stats,\n            mode=\"text\",\n            textposition=\"middle center\",\n            showlegend=False\n        ),\n        row=3, col=2\n    )\n    \n    fig.update_layout(height=800, showlegend=True, title_text=\"LLM Observability Dashboard\")\n    return fig\n\ndef chat_with_llm(message, history):\n    \"\"\"Chat interface with the LLM\"\"\"\n    response = llm.generate_response(message)\n    return response\n\ndef get_recent_traces():\n    \"\"\"Get recent traces from Langfuse (mock data for demo)\"\"\"\n    try:\n        # For demo purposes, we'll create mock trace data\n        # In a real implementation, you would use: traces = langfuse_client.get_traces(limit=10)\n        trace_data = []\n        for i in range(10):\n            trace_data.append({\n                'id': f\"trace-{i}\",\n                'name': f\"Generation {i}\",\n                'timestamp': (datetime.now() - timedelta(minutes=random.randint(1, 60))).isoformat(),\n                'user_id': f\"user-{random.randint(1, 5)}\",\n                'metadata': json.dumps({\"model\": MODEL_NAME, \"status\": \"completed\"})\n            })\n        \n        return pd.DataFrame(trace_data)\n    \n    except Exception as e:\n        return pd.DataFrame({'Error': [f\"Failed to fetch traces: {str(e)}\"]})\n\n# Create the Gradio interface\nwith gr.Blocks(title=\"LLM Observability Dashboard\", theme=gr.themes.Soft()) as demo:\n    gr.Markdown(\"# 🚀 LLM Observability Dashboard\")\n    gr.Markdown(\"Monitor your LLM performance with real-time metrics and analytics\")\n    \n    with gr.Tab(\"Live Metrics\"):\n        with gr.Row():\n            with gr.Column():\n                latency_plot = gr.Plot(label=\"Latency\", value=create_latency_chart())\n            with gr.Column():\n                throughput_plot = gr.Plot(label=\"Throughput\", value=create_throughput_chart())\n        \n        with gr.Row():\n            with gr.Column():\n                quality_plot = gr.Plot(label=\"Quality Scores\", value=create_quality_chart())\n            with gr.Column():\n                comprehensive_dash = gr.Plot(label=\"Comprehensive Dashboard\", value=create_comprehensive_dashboard())\n        \n        refresh_btn = gr.Button(\"🔄 Refresh Metrics\")\n    \n    with gr.Tab(\"Chat Interface\"):\n        gr.Markdown(\"### Test the LLM with real-time monitoring\")\n        chatbot = gr.ChatInterface(\n            fn=chat_with_llm,\n            title=\"LLM Chat with Observability\",\n            description=\"Chat with the model while monitoring performance metrics\"\n        )\n    \n    with gr.Tab(\"Langfuse Traces\"):\n        gr.Markdown(\"### Recent Traces from Langfuse\")\n        traces_table = gr.Dataframe(\n            label=\"Recent Traces\",\n            value=get_recent_traces,\n            every=30  # Refresh every 30 seconds\n        )\n        refresh_traces = gr.Button(\"🔄 Refresh Traces\")\n    \n    with gr.Tab(\"Model Info\"):\n        gr.Markdown(f\"### Model: {MODEL_NAME}\")\n        gr.Markdown(f\"**Device:** {llm.device}\")\n        gr.Markdown(f\"**Parameters:** ~{llm.model.num_parameters():,}\")\n        gr.Markdown(\"**Capabilities:** Text generation, conversation\")\n        \n        # Model statistics\n        stats_data = {\n            'Metric': ['Avg Latency', 'Peak Throughput', 'Error Rate', 'Avg Quality'],\n            'Value': ['0.8s', '200 req/hr', '2.5%', '0.85'],\n            'Status': ['✅ Good', '✅ Good', '⚠️ Warning', '✅ Good']\n        }\n        stats_df = pd.DataFrame(stats_data)\n        gr.Dataframe(stats_df, label=\"Current Performance\")\n    \n    # Refresh functionality\n    def refresh_all():\n        return [\n            create_latency_chart(),\n            create_throughput_chart(),\n            create_quality_chart(),\n            create_comprehensive_dashboard(),\n            get_recent_traces()\n        ]\n    \n    refresh_btn.click(\n        fn=refresh_all,\n        outputs=[latency_plot, throughput_plot, quality_plot, comprehensive_dash, traces_table]\n    )\n    \n    refresh_traces.click(\n        fn=get_recent_traces,\n        outputs=traces_table\n    )\n\n# Launch the dashboard\nif __name__ == \"__main__\":\n    # This will open in a new tab when run locally\n    demo.launch(\n        share=True,  # Set to True if you want a public link\n        server_name=\"localhost\",\n        server_port=8084,\n        show_error=True,\n        debug=True\n       \n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T11:29:59.122257Z","iopub.execute_input":"2025-08-28T11:29:59.123016Z","execution_failed":"2025-08-28T11:38:46.284Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://localhost:8084\n* Running on public URL: https://12a213a9b8da34a0df.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://12a213a9b8da34a0df.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\n\ndef greet(name, intensity):\n    return \"Hello, \" + name + \"!\" * int(intensity)\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", \"slider\"],\n    outputs=[\"text\"],\n)\n\ndemo.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T11:24:51.386981Z","iopub.execute_input":"2025-08-28T11:24:51.387521Z","iopub.status.idle":"2025-08-28T11:24:53.011072Z","shell.execute_reply.started":"2025-08-28T11:24:51.387499Z","shell.execute_reply":"2025-08-28T11:24:53.010333Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7861\nIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://4a3314cc5ba97c09f3.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://4a3314cc5ba97c09f3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}